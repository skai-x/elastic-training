{"version": 2, "width": 121, "height": 35, "timestamp": 1636014164, "env": {"SHELL": "/usr/bin/zsh", "TERM": "screen"}}
[0.25185, "o", "\u001b[1m\u001b[3m%\u001b[23m\u001b[1m\u001b[0m                                                                                                                        \r \r"]
[0.252476, "o", "\u001bk..ning-operator\u001b\\"]
[0.283973, "o", "\r\u001b[0m\u001b[23m\u001b[24m\u001b[J(base) \u001b[01;32m➜  \u001b[36mtraining-operator\u001b[00m \u001b[01;34mgit:(\u001b[31mpytorchelastic\u001b[34m) \u001b[33m✗\u001b[00m \u001b[K"]
[0.284073, "o", "\u001b[?1h\u001b="]
[0.284112, "o", "\u001b[?2004h"]
[3.228722, "o", "c"]
[3.229936, "o", "\bclear\b\b\b\b"]
[3.274783, "o", "\bca   \b\b\b"]
[3.276135, "o", "t ./examples/pytorch/elastic/imagenet/imagenet.yaml\u001b[51D"]
[3.702073, "o", "\u001b[1C"]
[4.379778, "o", "\u001b[50C"]
[5.317113, "o", "\u001b[?1l\u001b>\u001b[?2004l\r\r\n"]
[5.317591, "o", "\u001bkcat\u001b\\"]
[5.320713, "o", "apiVersion: \"kubeflow.org/v1\"\r\nkind: PyTorchJob\r\nmetadata:\r\n  name: elastic-example-imagenet\r\nspec:\r\n  elasticPolicy:\r\n    rdzvBackend: c10d\r\n    minReplicas: 1\r\n    maxReplicas: 2\r\n    maxRestarts: 100\r\n  pytorchReplicaSpecs:\r\n    Worker:\r\n      replicas: 2\r\n      restartPolicy: OnFailure\r\n      template:\r\n        spec:\r\n          containers:\r\n            - name: pytorch\r\n              image: gaocegege/pytorch-elastic-example-imagenet:1.0.0-sigterm\r\n              imagePullPolicy: IfNotPresent\r\n              env:\r\n              - name: LOGLEVEL\r\n                value: DEBUG\r\n              command:\r\n                - python\r\n                - -m\r\n                - torch.distributed.run\r\n                - /workspace/examples/imagenet.py\r\n                - \"--arch=resnet18\"\r\n                - \"--epochs=20\"\r\n                - \"--batch-size=32\"\r\n                - \"--workers=0\"\r\n                - \"/workspace/data/tiny-imagenet-200\"\r\n"]
[5.320858, "o", "\u001b[1m\u001b[3m%\u001b[23m\u001b[1m\u001b[0m                                                                                                                        \r \r"]
[5.320981, "o", "\u001bk..ning-operator\u001b\\"]
[5.350469, "o", "\r\u001b[0m\u001b[23m\u001b[24m\u001b[J(base) \u001b[01;32m➜  \u001b[36mtraining-operator\u001b[00m \u001b[01;34mgit:(\u001b[31mpytorchelastic\u001b[34m) \u001b[33m✗\u001b[00m \u001b[K"]
[5.35057, "o", "\u001b[?1h\u001b=\u001b[?2004h"]
[9.721406, "o", "k"]
[9.7227, "o", "\bkubectl delete pytorchjobs.kubeflow.org --all\u001b[44D"]
[9.834195, "o", "\u001b[1C"]
[10.025898, "o", "\u001b[1C"]
[10.057817, "o", "\u001b[1C"]
[10.134123, "o", "\u001b[1C"]
[10.640755, "o", "\u001b[1C"]
[10.762584, "o", "\u001b[1C"]
[11.054092, "o", "\u001b[1C"]
[11.26305, "o", "a                                    \u001b[36D"]
[11.264292, "o", "pply -f ./examples/pytorch/elastic/imagenet/imagenet.yaml\u001b[57D"]
[11.370494, "o", "\u001b[1C"]
[11.562035, "o", "\u001b[1C"]
[12.17633, "o", "\u001b[55C"]
[12.909543, "o", "\u001b[?1l\u001b>"]
[12.909665, "o", "\u001b[?2004l\r\r\n"]
[12.910018, "o", "\u001bkkubectl\u001b\\"]
[13.020062, "o", "pytorchjob.kubeflow.org/elastic-example-imagenet created\r\n"]
[13.024593, "o", "\u001b[1m\u001b[3m%\u001b[23m\u001b[1m\u001b[0m                                                                                                                        \r \r"]
[13.024716, "o", "\u001bk..ning-operator\u001b\\"]
[13.058663, "o", "\r\u001b[0m\u001b[23m\u001b[24m\u001b[J(base) \u001b[01;32m➜  \u001b[36mtraining-operator\u001b[00m \u001b[01;34mgit:(\u001b[31mpytorchelastic\u001b[34m) \u001b[33m✗\u001b[00m \u001b[K"]
[13.05879, "o", "\u001b[?1h\u001b=\u001b[?2004h"]
[15.574121, "o", "c"]
[15.575454, "o", "\bcat ./examples/pytorch/elastic/imagenet/imagenet.yaml\u001b[52D"]
[15.689209, "o", "\u001b[1C"]
[16.58053, "o", "\u001b[51C"]
[18.915666, "o", "\u001b[?2004l\r\r\n"]
[18.915971, "o", "\u001b[1m\u001b[3m%\u001b[23m\u001b[1m\u001b[0m                                                                                                                        \r \r"]
[18.916081, "o", "\u001bk..ning-operator\u001b\\"]
[18.947044, "o", "\r\u001b[0m\u001b[23m\u001b[24m\u001b[J(base) \u001b[01;31m➜  \u001b[36mtraining-operator\u001b[00m \u001b[01;34mgit:(\u001b[31mpytorchelastic\u001b[34m) \u001b[33m✗\u001b[00m \u001b[K"]
[18.947168, "o", "\u001b[?1h\u001b=\u001b[?2004h"]
[19.448872, "o", "e"]
[19.450131, "o", "\bexit\b\b\b"]
[19.5164, "o", "\u001b[1C"]
[19.612965, "o", "\u001b[1C"]
[19.857919, "o", "\u001b[1C"]
[20.285122, "o", "\b \b"]
[20.286389, "o", "t\b"]
[20.415908, "o", "\b  \b\b"]
[20.417171, "o", "it\b\b"]
[20.524444, "o", "\b\be   \b\b\b"]
[20.525582, "o", "\bexit\b\b\b"]
[20.620107, "o", "\b    \b\b\b\b"]
[20.953395, "o", "c"]
[20.954487, "o", "\bcat ./examples/pytorch/elastic/imagenet/imagenet.yaml\u001b[52D"]
[21.003917, "o", "\u001b[1C"]
[21.088959, "o", "\u001b[1C"]
[21.09231, "o", "g                                                 \u001b[49D"]
[21.093487, "o", " $HOME/.config/asciinema/install-id\u001b[35D"]
[21.966588, "o", "\b                                    \u001b[36D"]
[21.967838, "o", " ./examples/pytorch/elastic/imagenet/imagenet.yaml\u001b[50D"]
[22.519292, "o", "\u001b[50C"]
[22.929881, "o", "\u001b[?1l\u001b>\u001b[?2004l\r\r\n"]
[22.930449, "o", "\u001bkcat\u001b\\"]
[22.931592, "o", "apiVersion: \"kubeflow.org/v1\"\r\nkind: PyTorchJob\r\nmetadata:\r\n  name: elastic-example-imagenet\r\nspec:\r\n  elasticPolicy:\r\n    rdzvBackend: c10d\r\n    minReplicas: 1\r\n    maxReplicas: 2\r\n    maxRestarts: 100\r\n  pytorchReplicaSpecs:\r\n    Worker:\r\n      replicas: 2\r\n      restartPolicy: OnFailure\r\n      template:\r\n        spec:\r\n          containers:\r\n            - name: pytorch\r\n              image: gaocegege/pytorch-elastic-example-imagenet:1.0.0-sigterm\r\n              imagePullPolicy: IfNotPresent\r\n              env:\r\n              - name: LOGLEVEL\r\n                value: DEBUG\r\n              command:\r\n                - python\r\n                - -m\r\n                - torch.distributed.run\r\n                - /workspace/examples/imagenet.py\r\n                - \"--arch=resnet18\"\r\n                - \"--epochs=20\"\r\n                - \"--batch-size=32\"\r\n                - \"--workers=0\"\r\n                - \"/workspace/data/tiny-imagenet-200\"\r\n"]
[22.93175, "o", "\u001b[1m\u001b[3m%\u001b[23m\u001b[1m\u001b[0m                                                                                                                        \r \r"]
[22.931869, "o", "\u001bk..ning-operator\u001b\\"]
[22.961821, "o", "\r\u001b[0m\u001b[23m\u001b[24m\u001b[J(base) \u001b[01;32m➜  \u001b[36mtraining-operator\u001b[00m \u001b[01;34mgit:(\u001b[31mpytorchelastic\u001b[34m) \u001b[33m✗\u001b[00m \u001b[K"]
[22.961919, "o", "\u001b[?1h\u001b=\u001b[?2004h"]
[26.192671, "o", "k"]
[26.194017, "o", "\bkubectl apply -f ./examples/pytorch/elastic/imagenet/imagenet.yaml\u001b[65D"]
[26.43633, "o", "\u001b[1C"]
[26.719065, "o", "\u001b[1C"]
[26.847497, "o", "\u001b[1C"]
[27.128028, "o", "\u001b[1C"]
[27.434878, "o", "\u001b[1C"]
[27.539495, "o", "\u001b[1C"]
[27.844655, "o", "\u001b[1C"]
[27.891853, "o", "g                                                         \u001b[57D"]
[27.893162, "o", "et pods\b\b\b\b\b\b\b"]
[28.047742, "o", "\u001b[1C"]
[28.154574, "o", "\u001b[1C"]
[28.286933, "o", "\u001b[1C"]
[28.612851, "o", "s   \b\b\b"]
[28.614241, "o", "vc\b\b"]
[28.820133, "o", "\u001b[1C"]
[29.061163, "o", "\u001b[1C"]
[29.646114, "o", "\u001b[?1l\u001b>\u001b[?2004l\r\r\n"]
[29.64659, "o", "\u001bkkubectl\u001b\\"]
[29.693656, "o", "NAME                                TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)     AGE\r\nelastic-example-imagenet-worker-0   ClusterIP   None         <none>        23456/TCP   17s"]
[29.693805, "o", "\r\nelastic-example-imagenet-worker-1   ClusterIP   None         <none>        23456/TCP   17s\r\nkubernetes                          ClusterIP   10.96.0.1    <none>        443/TCP     6d22h\r\n"]
[29.695452, "o", "\u001b[1m\u001b[3m%\u001b[23m\u001b[1m\u001b[0m                                                                                                                        \r \r"]
[29.695557, "o", "\u001bk..ning-operator\u001b\\"]
[29.722539, "o", "\r\u001b[0m\u001b[23m\u001b[24m\u001b[J(base) \u001b[01;32m➜  \u001b[36mtraining-operator\u001b[00m \u001b[01;34mgit:(\u001b[31mpytorchelastic\u001b[34m) \u001b[33m✗\u001b[00m \u001b[K"]
[29.722647, "o", "\u001b[?1h\u001b=\u001b[?2004h"]
[31.325123, "o", "k"]
[31.326371, "o", "\bkubectl get svc\u001b[14D"]
[31.639177, "o", "\u001b[1C"]
[31.736903, "o", "\u001b[1C"]
[31.915218, "o", "\u001b[1C"]
[32.289751, "o", "\u001b[1C"]
[32.555122, "o", "\u001b[1C"]
[32.638134, "o", "\u001b[1C"]
[32.718271, "o", "\u001b[1C"]
[33.093073, "o", "\u001b[1C"]
[33.271643, "o", "\u001b[1C"]
[33.386875, "o", "\u001b[1C"]
[33.601087, "o", "\u001b[1C"]
[33.914145, "o", "p  \b\b"]
[33.915421, "o", "ods\b\b\b"]
[34.09022, "o", "\u001b[1C"]
[34.295698, "o", "\u001b[1Cds\b\b"]
[34.296696, "o", "s \b"]
[35.934536, "o", "\u001b[?1l\u001b>"]
[35.934674, "o", "\u001b[?2004l\r\r\n"]
[35.935029, "o", "\u001bkkubectl\u001b\\"]
[35.977108, "o", "NAME                                READY   STATUS    RESTARTS   AGE\r\nelastic-example-imagenet-worker-0   1/1     Running   0          23s\r\nelastic-example-imagenet-worker-1   1/1     Running   1          23s\r\n"]
[35.978923, "o", "\u001b[1m\u001b[3m%\u001b[23m\u001b[1m\u001b[0m                                                                                                                        \r \r"]
[35.97904, "o", "\u001bk..ning-operator\u001b\\"]
[36.008985, "o", "\r\u001b[0m\u001b[23m\u001b[24m\u001b[J(base) \u001b[01;32m➜  \u001b[36mtraining-operator\u001b[00m \u001b[01;34mgit:(\u001b[31mpytorchelastic\u001b[34m) \u001b[33m✗\u001b[00m \u001b[K"]
[36.009099, "o", "\u001b[?1h\u001b=\u001b[?2004h"]
[37.369549, "o", "k"]
[37.370854, "o", "\bkubectl get pods\u001b[15D"]
[37.675822, "o", "\u001b[1C"]
[37.789019, "o", "\u001b[1C"]
[38.091304, "o", "\u001b[1C"]
[38.397585, "o", "\u001b[1C"]
[38.585109, "o", "\u001b[1C"]
[38.802089, "o", "\u001b[1C"]
[39.00599, "o", "\u001b[1C"]
[39.313598, "o", "l       \b\b\b\b\b\b\b"]
[39.314908, "o", "ogs elastic-example-imagenet-worker-1\u001b[37D"]
[39.427854, "o", "\u001b[1C"]
[39.723118, "o", "\u001b[1C"]
[40.552319, "o", "\u001b[35C"]
[43.186957, "o", "\u001b[?1l\u001b>\u001b[?2004l\r\r\n"]
[43.187386, "o", "\u001bkkubectl\u001b\\"]
[43.250867, "o", "INFO:torch.distributed.launcher.api:Starting elastic_operator with launch configs:\r\n  entrypoint       : /workspace/examples/imagenet.py\r\n  min_nodes        : 1\r\n  max_nodes        : 2\r\n  nproc_per_node   : 1\r\n  run_id           : none\r\n  rdzv_backend     : c10d\r\n  rdzv_endpoint    : elastic-example-imagenet-worker-0:23456\r\n  rdzv_configs     : {'timeout': 900}\r\n  max_restarts     : 100\r\n  monitor_interval : 5\r\n  log_dir          : None\r\n"]
[43.25103, "o", "  metrics_cfg      : {}\r\n\r\nINFO:torch.distributed.elastic.agent.server.local_elastic_agent:log directory set to: /tmp/torchelastic_t0cc75rw/none_2nayzr1c\r\nINFO:torch.distributed.elastic.agent.server.api:[default] starting workers for entrypoint: python\r\nINFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous'ing worker group\r\nINFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous complete for workers. Result:\r\n  restart_count=0\r\n  master_addr=elastic-example-imagenet-worker-0\r\n  master_port=49951\r\n  group_rank=1\r\n  group_world_size=2\r\n  local_ranks=[0]\r\n  role_ranks=[1]\r\n  global_ranks=[1]\r\n  role_world_sizes=[2]\r\n  global_world_sizes=[2]\r\n\r\nINFO:torch.distributed.elastic.agent.server.api:[default] Starting worker group\r\nINFO:torch.distributed.elastic.multiprocessing:Setting worker0 reply file to: /tmp/torchelastic_t0cc75rw/none_2nayzr1c/attempt_0/0/error.json\r\nTraceback (most recent call last):\r\n  File \"/workspace/examples/imagenet.py\", line 580, in <module>\r\n    main()\r\n  File \"/o"]
[43.251092, "o", "pt/conda/lib/python3.7/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 345, in wrapper\r\n    return f(*args, **kwargs)\r\n  File \"/workspace/examples/imagenet.py\", line 155, in main\r\n    backend=args.dist_backend, init_method=\"env://\", timeout=timedelta(seconds=10)\r\n  File \"/opt/conda/lib/python3.7/site-packages/torch/distributed/distributed_c10d.py\", line 576, in init_process_group\r\n    store, rank, world_size = next(rendezvous_iterator)\r\n  File \"/opt/conda/lib/python3.7/site-packages/torch/distributed/rendezvous.py\", line 229, in _env_rendezvous_handler\r\n    store = _create_c10d_store(master_addr, master_port, rank, world_size, timeout)\r\n  File \"/opt/conda/lib/python3.7/site-packages/torch/distributed/rendezvous.py\", line 158, in _create_c10d_store\r\n    hostname, port, world_size, start_daemon, timeout, multi_tenant=True\r\nValueError: host not found: Temporary failure in name resolution\r\nDEBUG:torch.distributed.elastic.multiprocessing.errors:User process failed with error data:"]
[43.251131, "o", " {\r\n  \"message\": {\r\n    \"message\": \"ValueError: host not found: Temporary failure in name resolution\",\r\n    \"extraInfo\": {\r\n      \"py_callstack\": \"Traceback (most recent call last):\\n  File \\\"/opt/conda/lib/python3.7/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\\\", line 345, in wrapper\\n    return f(*args, **kwargs)\\n  File \\\"/workspace/examples/imagenet.py\\\", line 155, in main\\n    backend=args.dist_backend, init_method=\\\"env://\\\", timeout=timedelta(seconds=10)\\n  File \\\"/opt/conda/lib/python3.7/site-packages/torch/distributed/distributed_c10d.py\\\", line 576, in init_process_group\\n    store, rank, world_size = next(rendezvous_iterator)\\n  File \\\"/opt/conda/lib/python3.7/site-packages/torch/distributed/rendezvous.py\\\", line 229, in _env_rendezvous_handler\\n    store = _create_c10d_store(master_addr, master_port, rank, world_size, timeout)\\n  File \\\"/opt/conda/lib/python3.7/site-packages/torch/distributed/rendezvous.py\\\", line 158, in _create_c10d_store\\n    hostname, port, world_"]
[43.251168, "o", "size, start_daemon, timeout, multi_tenant=True\\nValueError: host not found: Temporary failure in name resolution\\n\",\r\n      \"timestamp\": \"1636014180\"\r\n    }\r\n  }\r\n}\r\nERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 11) of binary: /opt/conda/bin/python\r\nINFO:torch.distributed.elastic.agent.server.api:[default] Worker group FAILED. 100/100 attempts left; will restart worker group\r\nINFO:torch.distributed.elastic.agent.server.api:[default] Stopping worker group\r\nINFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous'ing worker group\r\n"]
[43.251465, "o", "INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous complete for workers. Result:\r\n  restart_count=1\r\n  master_addr=elastic-example-imagenet-worker-0\r\n  master_port=35023\r\n  group_rank=1\r\n  group_world_size=2\r\n  local_ranks=[0]\r\n  role_ranks=[1]\r\n"]
[43.25159, "o", "  global_ranks=[1]\r\n  role_world_sizes=[2]\r\n  global_world_sizes=[2]\r\n\r\nINFO:torch.distributed.elastic.agent.server.api:[default] Starting worker group\r\nINFO:torch.distributed.elastic.multiprocessing:Setting worker0 reply file to: /tmp/torchelastic_t0cc75rw/none_2nayzr1c/attempt_1/0/error.json\r\nTraceback (most recent call last):\r\n  File \"/workspace/examples/imagenet.py\", line 580, in <module>\r\n    main()\r\n  File \"/opt/conda/lib/python3.7/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 345, in wrapper\r\n    return f(*args, **kwargs)\r\n  File \"/workspace/examples/imagenet.py\", line 155, in main\r\n    backend=args.dist_backend, init_method=\"env://\", timeout=timedelta(seconds=10)\r\n  File \"/opt/conda/lib/python3.7/site-packages/torch/distributed/distributed_c10d.py\", line 576, in init_process_group\r\n    store, rank, world_size = next(rendezvous_iterator)\r\n  File \"/opt/conda/lib/python3.7/site-packages/torch/distributed/rendezvous.py\", line 229, in _env_rendezvous_handler\r\n    store ="]
[43.251669, "o", " _create_c10d_store(master_addr, master_port, rank, world_size, timeout)\r\n  File \"/opt/conda/lib/python3.7/site-packages/torch/distributed/rendezvous.py\", line 158, in _create_c10d_store\r\n    hostname, port, world_size, start_daemon, timeout, multi_tenant=True\r\nValueError: host not found: Temporary failure in name resolution\r\nDEBUG:torch.distributed.elastic.multiprocessing.errors:User process failed with error data: {\r\n  \"message\": {\r\n    \"message\": \"ValueError: host not found: Temporary failure in name resolution\",\r\n    \"extraInfo\": {\r\n      \"py_callstack\": \"Traceback (most recent call last):\\n  File \\\"/opt/conda/lib/python3.7/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\\\", line 345, in wrapper\\n    return f(*args, **kwargs)\\n  File \\\"/workspace/examples/imagenet.py\\\", line 155, in main\\n    backend=args.dist_backend, init_method=\\\"env://\\\", timeout=timedelta(seconds=10)\\n  File \\\"/opt/conda/lib/python3.7/site-packages/torch/distributed/distributed_c10d.py\\\", line 576, in init_p"]
[43.251705, "o", "rocess_group\\n    store, rank, world_size = next(rendezvous_iterator)\\n  File \\\"/opt/conda/lib/python3.7/site-packages/torch/distributed/rendezvous.py\\\", line 229, in _env_rendezvous_handler\\n    store = _create_c10d_store(master_addr, master_port, rank, world_size, timeout)\\n  File \\\"/opt/conda/lib/python3.7/site-packages/torch/distributed/rendezvous.py\\\", line 158, in _create_c10d_store\\n    hostname, port, world_size, start_daemon, timeout, multi_tenant=True\\nValueError: host not found: Temporary failure in name resolution\\n\",\r\n      \"timestamp\": \"1636014191\"\r\n    }\r\n  }\r\n}\r\nERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 15) of binary: /opt/conda/bin/python\r\nINFO:torch.distributed.elastic.agent.server.api:[default] Worker group FAILED. 99/100 attempts left; will restart worker group\r\nINFO:torch.distributed.elastic.agent.server.api:[default] Stopping worker group\r\nINFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous'ing worker group\r\nINFO:torch.d"]
[43.251778, "o", "istributed.elastic.agent.server.api:[default] Rendezvous complete for workers. Result:\r\n  restart_count=2\r\n  master_addr=elastic-example-imagenet-worker-0\r\n  master_port=51241\r\n  group_rank=1\r\n  group_world_size=2\r\n  local_ranks=[0]\r\n  role_ranks=[1]\r\n  global_ranks=[1]\r\n  role_world_sizes=[2]\r\n  global_world_sizes=[2]\r\n\r\nINFO:torch.distributed.elastic.agent.server.api:[default] Starting worker group\r\nINFO:torch.distributed.elastic.multiprocessing:Setting worker0 reply file to: /tmp/torchelastic_t0cc75rw/none_2nayzr1c/attempt_2/0/error.json\r\nTraceback (most recent call last):\r\n  File \"/workspace/examples/imagenet.py\", line 580, in <module>\r\n    main()\r\n  File \"/opt/conda/lib/python3.7/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 345, in wrapper\r\n    return f(*args, **kwargs)\r\n"]
[43.251826, "o", "  File \"/workspace/examples/imagenet.py\", line 155, in main\r\n    backend=args.dist_backend, init_method=\"env://\", timeout=timedelta(seconds=10)\r\n  File \"/opt/conda/lib/python3.7/site-packages/torch/distributed/distributed_c10d.py\", line 576, in init_process_group\r\n    store, rank, world_size = next(rendezvous_iterator)\r\n  File \"/opt/conda/lib/python3.7/site-packages/torch/distributed/rendezvous.py\", line 229, in _env_rendezvous_handler\r\n    store = _create_c10d_store(master_addr, master_port, rank, world_size, timeout)\r\n  File \"/opt/conda/lib/python3.7/site-packages/torch/distributed/rendezvous.py\", line 158, in _create_c10d_store\r\n    hostname, port, world_size, start_daemon, timeout, multi_tenant=True\r\nValueError: host not found: Temporary failure in name resolution\r\nDEBUG:torch.distributed.elastic.multiprocessing.errors:User process failed with error data: {\r\n  \"message\": {\r\n    \"message\": \"ValueError: host not found: Temporary failure in name resolution\",\r\n    \"extraInfo\": {\r\n      \"py_callstack\": \"Traceb"]
[43.251878, "o", "ack (most recent call last):\\n  File \\\"/opt/conda/lib/python3.7/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\\\", line 345, in wrapper\\n    return f(*args, **kwargs)\\n  File \\\"/workspace/examples/imagenet.py\\\", line 155, in main\\n    backend=args.dist_backend, init_method=\\\"env://\\\", timeout=timedelta(seconds=10)\\n  File \\\"/opt/conda/lib/python3.7/site-packages/torch/distributed/distributed_c10d.py\\\", line 576, in init_process_group\\n    store, rank, world_size = next(rendezvous_iterator)\\n  File \\\"/opt/conda/lib/python3.7/site-packages/torch/distributed/rendezvous.py\\\", line 229, in _env_rendezvous_handler\\n    store = _create_c10d_store(master_addr, master_port, rank, world_size, timeout)\\n  File \\\"/opt/conda/lib/python3.7/site-packages/torch/distributed/rendezvous.py\\\", line 158, in _create_c10d_store\\n    hostname, port, world_size, start_daemon, timeout, multi_tenant=True\\nValueError: host not found: Temporary failure in name resolution\\n\",\r\n      \"timestamp\": \"1636014202\"\r\n  "]
[43.251901, "o", "  }\r\n  }\r\n}\r\nERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 19) of binary: /opt/conda/bin/python\r\nINFO:torch.distributed.elastic.agent.server.api:[default] Worker group FAILED. 98/100 attempts left; will restart worker group\r\nINFO:torch.distributed.elastic.agent.server.api:[default] Stopping worker group\r\nINFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous'ing worker group\r\n"]
[43.253701, "o", "\u001b[1m\u001b[3m%\u001b[23m\u001b[1m\u001b[0m                                                                                                                        \r \r"]
[43.253837, "o", "\u001bk..ning-operator\u001b\\"]
[43.283873, "o", "\r\u001b[0m\u001b[23m\u001b[24m\u001b[J(base) \u001b[01;32m➜  \u001b[36mtraining-operator\u001b[00m \u001b[01;34mgit:(\u001b[31mpytorchelastic\u001b[34m) \u001b[33m✗\u001b[00m \u001b[K"]
[43.283983, "o", "\u001b[?1h\u001b=\u001b[?2004h"]
[48.018583, "o", "kubectl logs elastic-example-imagenet-worker-1"]
[49.758127, "o", "\b \b"]
[49.759224, "o", "1\b"]
[50.181091, "o", "0"]
[50.481818, "o", "\u001b[?1l\u001b>\u001b[?2004l\r\r\n\u001bkkubectl\u001b\\"]
[50.564103, "o", "INFO:torch.distributed.launcher.api:Starting elastic_operator with launch configs:\r\n  entrypoint       : /workspace/examples/imagenet.py\r\n  min_nodes        : 1\r\n  max_nodes        : 2\r\n  nproc_per_node   : 1\r\n  run_id           : none\r\n  rdzv_backend     : c10d\r\n  rdzv_endpoint    : elastic-example-imagenet-worker-0:23456\r\n  rdzv_configs     : {'timeout': 900}\r\n  max_restarts     : 100\r\n  monitor_interval : 5\r\n  log_dir          : None\r\n  metrics_cfg      : {}\r\n\r\nINFO:torch.distributed.elastic.agent.server.local_elastic_agent:log directory set to: /tmp/torchelastic_m8me90uk/none_qs2bblei\r\nINFO:torch.distributed.elastic.agent.server.api:[default] starting workers for entrypoint: python\r\nINFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous'ing worker group\r\nINFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous complete for workers. Result:\r\n  restart_count=0\r\n  master_addr=elastic-example-imagenet-worker-0\r\n  master_port=49951\r\n  group_rank=0\r\n  group_world_size=2\r\n  local_ranks="]
[50.564173, "o", "[0]\r\n  role_ranks=[0]\r\n  global_ranks=[0]\r\n  role_world_sizes=[2]\r\n  global_world_sizes=[2]\r\n\r\nINFO:torch.distributed.elastic.agent.server.api:[default] Starting worker group\r\nINFO:torch.distributed.elastic.multiprocessing:Setting worker0 reply file to: /tmp/torchelastic_m8me90uk/none_qs2bblei/attempt_0/0/error.json\r\nINFO:torch.distributed.elastic.agent.server.api:[default] Detected 1 new nodes from group_rank=0; will restart worker group\r\nINFO:torch.distributed.elastic.agent.server.api:[default] Stopping worker group\r\nWARNING:torch.distributed.elastic.multiprocessing.api:Sending process 12 closing signal SIGTERM\r\nINFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous'ing worker group\r\nINFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous complete for workers. Result:\r\n  restart_count=0\r\n  master_addr=elastic-example-imagenet-worker-0\r\n  master_port=35023\r\n  group_rank=0\r\n  group_world_size=2\r\n  local_ranks=[0]\r\n  role_ranks=[0]\r\n  global_ranks=[0]\r\n  role_world_sizes=[2]\r\n  globa"]
[50.564186, "o", "l_world_sizes=[2]\r\n\r\nINFO:torch.distributed.elastic.agent.server.api:[default] Starting worker group\r\nINFO:torch.distributed.elastic.multiprocessing:Setting worker0 reply file to: /tmp/torchelastic_m8me90uk/none_qs2bblei/attempt_0/0/error.json\r\nINFO:torch.distributed.elastic.agent.server.api:[default] Detected 1 new nodes from group_rank=0; will restart worker group\r\nINFO:torch.distributed.elastic.agent.server.api:[default] Stopping worker group\r\nWARNING:torch.distributed.elastic.multiprocessing.api:Sending process 17 closing signal SIGTERM\r\nINFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous'ing worker group\r\nINFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous complete for workers. Result:\r\n  restart_count=0\r\n  master_addr=elastic-example-imagenet-worker-0\r\n  master_port=51241\r\n  group_rank=0\r\n  group_world_size=2\r\n  local_ranks=[0]\r\n  role_ranks=[0]\r\n  global_ranks=[0]\r\n  role_world_sizes=[2]\r\n  global_world_sizes=[2]\r\n\r\nINFO:torch.distributed.elastic.agent.server.api:[defa"]
[50.564205, "o", "ult] Starting worker group\r\nINFO:torch.distributed.elastic.multiprocessing:Setting worker0 reply file to: /tmp/torchelastic_m8me90uk/none_qs2bblei/attempt_0/0/error.json\r\nINFO:torch.distributed.elastic.agent.server.api:[default] Detected 1 new nodes from group_rank=0; will restart worker group\r\nINFO:torch.distributed.elastic.agent.server.api:[default] Stopping worker group\r\nWARNING:torch.distributed.elastic.multiprocessing.api:Sending process 22 closing signal SIGTERM\r\nINFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous'ing worker group\r\nINFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous complete for workers. Result:\r\n  restart_count=0\r\n  master_addr=elastic-example-imagenet-worker-0\r\n  master_port=52111\r\n  group_rank=0\r\n  group_world_size=2\r\n  local_ranks=[0]\r\n  role_ranks=[0]\r\n  global_ranks=[0]\r\n  role_world_sizes=[2]\r\n  global_world_sizes=[2]\r\n\r\nINFO:torch.distributed.elastic.agent.server.api:[default] Starting worker group\r\n"]
[50.564443, "o", "INFO:torch.distributed.elastic.multiprocessing:Setting worker0 reply file to: /tmp/torchelastic_m8me90uk/none_qs2bblei/attempt_0/0/error.json\r\n=> creating model: resnet18\r\n=> no workers have checkpoints, starting from epoch 0\r\n=> start_epoch: 0, best_acc1: 0\r\n"]
[50.569948, "o", "\u001b[1m\u001b[3m%\u001b[23m\u001b[1m\u001b[0m                                                                                                                        \r \r\u001bk..ning-operator\u001b\\"]
[50.62622, "o", "\r\u001b[0m\u001b[23m\u001b[24m\u001b[J(base) \u001b[01;32m➜  \u001b[36mtraining-operator\u001b[00m \u001b[01;34mgit:(\u001b[31mpytorchelastic\u001b[34m) \u001b[33m✗\u001b[00m \u001b[K"]
[50.626613, "o", "\u001b[?1h\u001b=\u001b[?2004h"]
[54.062259, "o", "kubectl logs elastic-example-imagenet-worker-0"]
[55.496409, "o", "\u001b[?2004l\r\r\n\u001b[1m\u001b[3m%\u001b[23m\u001b[1m\u001b[0m                                                                                                                        \r \r\u001bk..ning-operator\u001b\\"]
[55.608315, "o", "\r\u001b[0m\u001b[23m\u001b[24m\u001b[J(base) \u001b[01;31m➜  \u001b[36mtraining-operator\u001b[00m \u001b[01;34mgit:(\u001b[31mpytorchelastic\u001b[34m) \u001b[33m✗\u001b[00m \u001b[K"]
[55.608501, "o", "\u001b[?1h\u001b=\u001b[?2004h"]
[55.662438, "o", "k"]
[55.66386, "o", "\bkubectl logs elastic-example-imagenet-worker-0\u001b[45D"]
[55.860252, "o", "\u001b[1C"]
[56.004523, "o", "\u001b[1C"]
[56.120614, "o", "\u001b[1C"]
[56.217853, "o", "\u001b[1C"]
[56.515965, "o", "\u001b[1C"]
[56.590602, "o", "\u001b[1C"]
[56.706891, "o", "\u001b[1C"]
[56.928721, "o", "\u001b[1C"]
[56.992986, "o", "\u001b[1C"]
[57.196862, "o", "\u001b[1C"]
[57.284015, "o", "\u001b[1C"]
[57.352508, "o", "\u001b[1C"]
[57.470601, "o", "-                                \u001b[32D"]
[57.471845, "o", "f elastic-example-imagenet-worker-0\u001b[35D"]
[57.677645, "o", "\u001b[1C"]
[57.745488, "o", "\u001b[1C"]
[58.267727, "o", "\u001b[33C"]
[58.986827, "o", "\u001b[?1l\u001b>\u001b[?2004l\r\r\n\u001bkkubectl\u001b\\"]
[59.104792, "o", "INFO:torch.distributed.launcher.api:Starting elastic_operator with launch configs:\r\n  entrypoint       : /workspace/examples/imagenet.py\r\n  min_nodes        : 1\r\n  max_nodes        : 2\r\n  nproc_per_node   : 1\r\n  run_id           : none\r\n  rdzv_backend     : c10d\r\n  rdzv_endpoint    : elastic-example-imagenet-worker-0:23456\r\n  rdzv_configs     : {'timeout': 900}\r\n  max_restarts     : 100\r\n  monitor_interval : 5\r\n  log_dir          : None\r\n  metrics_cfg      : {}\r\n\r\n"]
[59.105524, "o", "INFO:torch.distributed.elastic.agent.server.local_elastic_agent:log directory set to: /tmp/torchelastic_m8me90uk/none_qs2bblei\r\n"]
[59.105758, "o", "INFO:torch.distributed.elastic.agent.server.api:[default] starting workers for entrypoint: python\r\nINFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous'ing worker group\r\nINFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous complete for workers. Result:\r\n  restart_count=0\r\n"]
[59.105911, "o", "  master_addr=elastic-example-imagenet-worker-0\r\n  master_port=49951\r\n  group_rank=0\r\n  group_world_size=2\r\n  local_ranks=[0]\r\n  role_ranks=[0]\r\n  global_ranks=[0]\r\n  role_world_sizes=[2]\r\n  global_world_sizes=[2]\r\n\r\nINFO:torch.distributed.elastic.agent.server.api:[default] Starting worker group\r\nINFO:torch.distributed.elastic.multiprocessing:Setting worker0 reply file to: /tmp/torchelastic_m8me90uk/none_qs2bblei/attempt_0/0/error.json\r\nINFO:torch.distributed.elastic.agent.server.api:[default] Detected 1 new nodes from group_rank=0; will restart worker group\r\nINFO:torch.distributed.elastic.agent.server.api:[default] Stopping worker group\r\nWARNING:torch.distributed.elastic.multiprocessing.api:Sending process 12 closing signal SIGTERM\r\nINFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous'ing worker group\r\nINFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous complete for workers. Result:\r\n  restart_count=0\r\n  master_addr=elastic-example-imagenet-worker-0\r\n  master_port=35023\r\n  gr"]
[59.105927, "o", "oup_rank=0\r\n  group_world_size=2\r\n  local_ranks=[0]\r\n  role_ranks=[0]\r\n  global_ranks=[0]\r\n"]
[59.106122, "o", "  role_world_sizes=[2]\r\n  global_world_sizes=[2]\r\n\r\nINFO:torch.distributed.elastic.agent.server.api:[default] Starting worker group\r\nINFO:torch.distributed.elastic.multiprocessing:Setting worker0 reply file to: /tmp/torchelastic_m8me90uk/none_qs2bblei/attempt_0/0/error.json\r\nINFO:torch.distributed.elastic.agent.server.api:[default] Detected 1 new nodes from group_rank=0; will restart worker group\r\nINFO:torch.distributed.elastic.agent.server.api:[default] Stopping worker group\r\n"]
[59.106163, "o", "WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 17 closing signal SIGTERM\r\n"]
[59.106255, "o", "INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous'ing worker group\r\nINFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous complete for workers. Result:\r\n"]
[59.106291, "o", "  restart_count=0\r\n"]
[59.106331, "o", "  master_addr=elastic-example-imagenet-worker-0\r\n"]
[59.106478, "o", "  master_port=51241\r\n"]
[59.106512, "o", "  group_rank=0\r\n  group_world_size=2\r\n  local_ranks=[0]\r\n"]
[59.106538, "o", "  role_ranks=[0]\r\n"]
[59.106562, "o", "  global_ranks=[0]\r\n"]
[59.106585, "o", "  role_world_sizes=[2]\r\n"]
[59.106637, "o", "  global_world_sizes=[2]\r\n"]
[59.106662, "o", "\r\n"]
[59.106691, "o", "INFO:torch.distributed.elastic.agent.server.api:[default] Starting worker group\r\n"]
[59.106726, "o", "INFO:torch.distributed.elastic.multiprocessing:Setting worker0 reply file to: /tmp/torchelastic_m8me90uk/none_qs2bblei/attempt_0/0/error.json\r\n"]
[59.106771, "o", "INFO:torch.distributed.elastic.agent.server.api:[default] Detected 1 new nodes from group_rank=0; will restart worker group\r\n"]
[59.106845, "o", "INFO:torch.distributed.elastic.agent.server.api:[default] Stopping worker group\r\n"]
[59.107142, "o", "WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 22 closing signal SIGTERM\r\nINFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous'ing worker group\r\nINFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous complete for workers. Result:\r\n  restart_count=0\r\n  master_addr=elastic-example-imagenet-worker-0\r\n  master_port=52111\r\n  group_rank=0\r\n  group_world_size=2\r\n"]
[59.107168, "o", "  local_ranks=[0]\r\n"]
[59.107348, "o", "  role_ranks=[0]\r\n  global_ranks=[0]\r\n  role_world_sizes=[2]\r\n  global_world_sizes=[2]\r\n\r\n"]
[59.107416, "o", "INFO:torch.distributed.elastic.agent.server.api:[default] Starting worker group\r\n"]
[59.107481, "o", "INFO:torch.distributed.elastic.multiprocessing:Setting worker0 reply file to: /tmp/torchelastic_m8me90uk/none_qs2bblei/attempt_0/0/error.json\r\n=> creating model: resnet18\r\n"]
[59.110086, "o", "=> no workers have checkpoints, starting from epoch 0\r\n=> start_epoch: 0, best_acc1: 0\r\nEpoch: [0][   0/1563]\tTime  2.085 ( 2.085)\tData  0.097 ( 0.097)\tLoss 7.0790e+00 (7.0790e+00)\tAcc@1   0.00 (  0.00)\tAcc@5   0.00 (  0.00)\r\n"]
[69.01054, "o", "^C"]
[69.012173, "o", "\r\n"]
[69.012354, "o", "\u001b[1m\u001b[3m%\u001b[23m\u001b[1m\u001b[0m                                                                                                                        \r \r\u001bk..ning-operator\u001b\\"]
[69.095741, "o", "\r\u001b[0m\u001b[23m\u001b[24m\u001b[J(base) \u001b[01;31m➜  \u001b[36mtraining-operator\u001b[00m \u001b[01;34mgit:(\u001b[31mpytorchelastic\u001b[34m) \u001b[33m✗\u001b[00m \u001b[K"]
[69.096182, "o", "\u001b[?1h\u001b=\u001b[?2004h"]
[69.797597, "o", "kubectl logs -f elastic-example-imagenet-worker-0"]
[70.248183, "o", "\b \b"]
[70.249405, "o", "0\b"]
[70.65052, "o", "1"]
[71.813587, "o", "\u001b[?1l\u001b>"]
[71.814964, "o", "\u001b[?2004l\r\r\n\u001bkkubectl\u001b\\"]
[71.8964, "o", "INFO:torch.distributed.launcher.api:Starting elastic_operator with launch configs:\r\n  entrypoint       : /workspace/examples/imagenet.py\r\n  min_nodes        : 1\r\n  max_nodes        : 2\r\n  nproc_per_node   : 1\r\n  run_id           : none\r\n  rdzv_backend     : c10d\r\n"]
[71.896736, "o", "  rdzv_endpoint    : elastic-example-imagenet-worker-0:23456\r\n"]
[71.900438, "o", "  rdzv_configs     : {'timeout': 900}\r\n  max_restarts     : 100\r\n  monitor_interval : 5\r\n  log_dir          : None\r\n  metrics_cfg      : {}\r\n"]
[71.900722, "o", "\r\nINFO:torch.distributed.elastic.agent.server.local_elastic_agent:log directory set to: /tmp/torchelastic_t0cc75rw/none_2nayzr1c\r\nINFO:torch.distributed.elastic.agent.server.api:[default] starting workers for entrypoint: python\r\nINFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous'ing worker group\r\nINFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous complete for workers. Result:\r\n  restart_count=0\r\n  master_addr=elastic-example-imagenet-worker-0\r\n  master_port=49951\r\n  group_rank=1\r\n  group_world_size=2\r\n  local_ranks=[0]\r\n  role_ranks=[1]\r\n  global_ranks=[1]\r\n  role_world_sizes=[2]\r\n  global_world_sizes=[2]\r\n\r\nINFO:torch.distributed.elastic.agent.server.api:[default] Starting worker group\r\nINFO:torch.distributed.elastic.multiprocessing:Setting worker0 reply file to: /tmp/torchelastic_t0cc75rw/none_2nayzr1c/attempt_0/0/error.json\r\nTraceback (most recent call last):\r\n  File \"/workspace/examples/imagenet.py\", line 580, in <module>\r\n    main()\r\n  File \"/opt/conda/lib/python3.7/si"]
[71.90074, "o", "te-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 345, in wrapper\r\n    return f(*args, **kwargs)\r\n  File \"/workspace/examples/imagenet.py\", line 155, in main\r\n    backend=args.dist_backend, init_method=\"env://\", timeout=timedelta(seconds=10)\r\n  File \"/opt/conda/lib/python3.7/site-packages/torch/distributed/distributed_c10d.py\", line 576, in init_process_group\r\n    store, rank, world_size = next(rendezvous_iterator)\r\n  File \"/opt/conda/lib/python3.7/site-packages/torch/distributed/rendezvous.py\", line 229, in _env_rendezvous_handler\r\n    store = _create_c10d_store(master_addr, master_port, rank, world_size, timeout)\r\n  File \"/opt/conda/lib/python3.7/site-packages/torch/distributed/rendezvous.py\", line 158, in _create_c10d_store\r\n    hostname, port, world_size, start_daemon, timeout, multi_tenant=True\r\nValueError: host not found: Temporary failure in name resolution\r\nDEBUG:torch.distributed.elastic.multiprocessing.errors:User process failed with error data: {\r\n  \"message\": {\r\n    \""]
[71.900749, "o", "message\": \"ValueError: host not found: Temporary failure in name resolution\",\r\n    \"extraInfo\": {\r\n      \"py_callstack\": \"Traceback (most recent call last):\\n  File \\\"/opt/conda/lib/python3.7/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\\\", line 345, in wrapper\\n    return f(*args, **kwargs)\\n  File \\\"/workspace/examples/imagenet.py\\\", line 155, in main\\n    backend=args.dist_backend, init_method=\\\"env://\\\", timeout=timedelta(seconds=10)\\n  File \\\"/opt/conda/lib/python3.7/site-packages/torch/distributed/distributed_c10d.py\\\", line 576, in init_process_group\\n    store, rank, world_size = next(rendezvous_iterator)\\n  File \\\"/opt/conda/lib/python3.7/site-packages/torch/distributed/rendezvous.py\\\", line 229, in _env_rendezvous_handler\\n    store = _create_c10d_store(master_addr, master_port, rank, world_size, timeout)\\n  File \\\"/opt/conda/lib/python3.7/site-packages/torch/distributed/rendezvous.py\\\", line 158, in _create_c10d_store\\n    hostname, port, world_size, start_daemon, timeo"]
[71.900758, "o", "ut, multi_tenant=True\\nValueError: host not found: Temporary failure in name resolution\\n\",\r\n      \"timestamp\": \"1636014180\"\r\n    }\r\n  }\r\n}\r\nERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 11) of binary: /opt/conda/bin/python\r\nINFO:torch.distributed.elastic.agent.server.api:[default] Worker group FAILED. 100/100 attempts left; will restart worker group\r\nINFO:torch.distributed.elastic.agent.server.api:[default] Stopping worker group\r\nINFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous'ing worker group\r\nINFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous complete for workers. Result:\r\n  restart_count=1\r\n  master_addr=elastic-example-imagenet-worker-0\r\n  master_port=35023\r\n  group_rank=1\r\n  group_world_size=2\r\n  local_ranks=[0]\r\n  role_ranks=[1]\r\n  global_ranks=[1]\r\n  role_world_sizes=[2]\r\n  global_world_sizes=[2]\r\n\r\nINFO:torch.distributed.elastic.agent.server.api:[default] Starting worker group\r\nINFO:torch.distributed.elastic.multip"]
[71.901309, "o", "rocessing:Setting worker0 reply file to: /tmp/torchelastic_t0cc75rw/none_2nayzr1c/attempt_1/0/error.json\r\nTraceback (most recent call last):\r\n  File \"/workspace/examples/imagenet.py\", line 580, in <module>\r\n    main()\r\n  File \"/opt/conda/lib/python3.7/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 345, in wrapper\r\n    return f(*args, **kwargs)\r\n  File \"/workspace/examples/imagenet.py\", line 155, in main\r\n    backend=args.dist_backend, init_method=\"env://\", timeout=timedelta(seconds=10)\r\n  File \"/opt/conda/lib/python3.7/site-packages/torch/distributed/distributed_c10d.py\", line 576, in init_process_group\r\n    store, rank, world_size = next(rendezvous_iterator)\r\n  File \"/opt/conda/lib/python3.7/site-packages/torch/distributed/rendezvous.py\", line 229, in _env_rendezvous_handler\r\n    store = _create_c10d_store(master_addr, master_port, rank, world_size, timeout)\r\n  File \"/opt/conda/lib/python3.7/site-packages/torch/distributed/rendezvous.py\", line 158, in _create_c10d_store\r\n  "]
[71.901325, "o", "  hostname, port, world_size, start_daemon, timeout, multi_tenant=True\r\nValueError: host not found: Temporary failure in name resolution\r\nDEBUG:torch.distributed.elastic.multiprocessing.errors:User process failed with error data: {\r\n  \"message\": {\r\n    \"message\": \"ValueError: host not found: Temporary failure in name resolution\",\r\n    \"extraInfo\": {\r\n      \"py_callstack\": \"Traceback (most recent call last):\\n  File \\\"/opt/conda/lib/python3.7/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\\\", line 345, in wrapper\\n    return f(*args, **kwargs)\\n  File \\\"/workspace/examples/imagenet.py\\\", line 155, in main\\n    backend=args.dist_backend, init_method=\\\"env://\\\", timeout=timedelta(seconds=10)\\n  File \\\"/opt/conda/lib/python3.7/site-packages/torch/distributed/distributed_c10d.py\\\", line 576, in init_process_group\\n    store, rank, world_size = next(rendezvous_iterator)\\n  File \\\"/opt/conda/lib/python3.7/site-packages/torch/distributed/rendezvous.py\\\", line 229, in _env_rendezvous_handler"]
[71.901333, "o", "\\n    store = _create_c10d_store(master_addr, master_port, rank, world_size, timeout)\\n  File \\\"/opt/conda/lib/python3.7/site-packages/torch/distributed/rendezvous.py\\\", line 158, in _create_c10d_store\\n    hostname, port, world_size, start_daemon, timeout, multi_tenant=True\\nValueError: host not found: Temporary failure in name resolution\\n\",\r\n      \"timestamp\": \"1636014191\"\r\n    }\r\n  }\r\n}\r\nERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 15) of binary: /opt/conda/bin/python\r\nINFO:torch.distributed.elastic.agent.server.api:[default] Worker group FAILED. 99/100 attempts left; will restart worker group\r\nINFO:torch.distributed.elastic.agent.server.api:[default] Stopping worker group\r\nINFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous'ing worker group\r\nINFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous complete for workers. Result:\r\n  restart_count=2\r\n  master_addr=elastic-example-imagenet-worker-0\r\n  master_port=51241\r\n  group_rank="]
[71.901341, "o", "1\r\n  group_world_size=2\r\n  local_ranks=[0]\r\n  role_ranks=[1]\r\n  global_ranks=[1]\r\n  role_world_sizes=[2]\r\n  global_world_sizes=[2]\r\n\r\nINFO:torch.distributed.elastic.agent.server.api:[default] Starting worker group\r\nINFO:torch.distributed.elastic.multiprocessing:Setting worker0 reply file to: /tmp/torchelastic_t0cc75rw/none_2nayzr1c/attempt_2/0/error.json\r\nTraceback (most recent call last):\r\n  File \"/workspace/examples/imagenet.py\", line 580, in <module>\r\n    main()\r\n  File \"/opt/conda/lib/python3.7/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 345, in wrapper\r\n    return f(*args, **kwargs)\r\n  File \"/workspace/examples/imagenet.py\", line 155, in main\r\n    backend=args.dist_backend, init_method=\"env://\", timeout=timedelta(seconds=10)\r\n  File \"/opt/conda/lib/python3.7/site-packages/torch/distributed/distributed_c10d.py\", line 576, in init_process_group\r\n    store, rank, world_size = next(rendezvous_iterator)\r\n  File \"/opt/conda/lib/python3.7/site-packages/torch/distributed/re"]
[71.901585, "o", "ndezvous.py\", line 229, in _env_rendezvous_handler\r\n    store = _create_c10d_store(master_addr, master_port, rank, world_size, timeout)\r\n  File \"/opt/conda/lib/python3.7/site-packages/torch/distributed/rendezvous.py\", line 158, in _create_c10d_store\r\n    hostname, port, world_size, start_daemon, timeout, multi_tenant=True\r\nValueError: host not found: Temporary failure in name resolution\r\nDEBUG:torch.distributed.elastic.multiprocessing.errors:User process failed with error data: {\r\n  \"message\": {\r\n    \"message\": \"ValueError: host not found: Temporary failure in name resolution\",\r\n    \"extraInfo\": {\r\n      \"py_callstack\": \"Traceback (most recent call last):\\n  File \\\"/opt/conda/lib/python3.7/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\\\", line 345, in wrapper\\n    return f(*args, **kwargs)\\n  File \\\"/workspace/examples/imagenet.py\\\", line 155, in main\\n    backend=args.dist_backend, init_method=\\\"env://\\\", timeout=timedelta(seconds=10)\\n  File \\\"/opt/conda/lib/python3.7/site-packag"]
[71.901602, "o", "es/torch/distributed/distributed_c10d.py\\\", line 576, in init_process_group\\n    store, rank, world_size = next(rendezvous_iterator)\\n  File \\\"/opt/conda/lib/python3.7/site-packages/torch/distributed/rendezvous.py\\\", line 229, in _env_rendezvous_handler\\n    store = _create_c10d_store(master_addr, master_port, rank, world_size, timeout)\\n  File \\\"/opt/conda/lib/python3.7/site-packages/torch/distributed/rendezvous.py\\\", line 158, in _create_c10d_store\\n    hostname, port, world_size, start_daemon, timeout, multi_tenant=True\\nValueError: host not found: Temporary failure in name resolution\\n\",\r\n      \"timestamp\": \"1636014202\"\r\n    }\r\n  }\r\n}\r\nERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 19) of binary: /opt/conda/bin/python\r\nINFO:torch.distributed.elastic.agent.server.api:[default] Worker group FAILED. 98/100 attempts left; will restart worker group\r\nINFO:torch.distributed.elastic.agent.server.api:[default] Stopping worker group\r\nINFO:torch.distributed.elastic.agent"]
[71.90161, "o", ".server.api:[default] Rendezvous'ing worker group\r\nINFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous complete for workers. Result:\r\n  restart_count=3\r\n  master_addr=elastic-example-imagenet-worker-0\r\n  master_port=52111\r\n  group_rank=1\r\n  group_world_size=2\r\n  local_ranks=[0]\r\n  role_ranks=[1]\r\n  global_ranks=[1]\r\n  role_world_sizes=[2]\r\n  global_world_sizes=[2]\r\n\r\nINFO:torch.distributed.elastic.agent.server.api:[default] Starting worker group\r\nINFO:torch.distributed.elastic.multiprocessing:Setting worker0 reply file to: /tmp/torchelastic_t0cc75rw/none_2nayzr1c/attempt_3/0/error.json\r\n=> creating model: resnet18\r\n=> no workers have checkpoints, starting from epoch 0\r\n=> start_epoch: 0, best_acc1: 0\r\nEpoch: [0][   0/1563]\tTime  2.080 ( 2.080)\tData  0.091 ( 0.091)\tLoss 6.9504e+00 (6.9504e+00)\tAcc@1   0.00 (  0.00)\tAcc@5   3.12 (  3.12)\r\nEpoch: [0][  10/1563]\tTime  1.964 ( 1.946)\tData  0.048 ( 0.057)\tLoss 7.2626e+00 (6.6883e+00)\tAcc@1   0.00 (  0.57)\tAcc@5   3.12 (  2.56)\r\n"]
[78.842559, "o", "^C"]
[78.84393, "o", "\r\n"]
[78.844007, "o", "\u001b[1m\u001b[3m%\u001b[23m\u001b[1m\u001b[0m                                                                                                                        \r \r"]
[78.844098, "o", "\u001bk..ning-operator\u001b\\"]
[78.947003, "o", "\r\u001b[0m\u001b[23m\u001b[24m\u001b[J(base) \u001b[01;31m➜  \u001b[36mtraining-operator\u001b[00m \u001b[01;34mgit:(\u001b[31mpytorchelastic\u001b[34m) \u001b[33m✗\u001b[00m \u001b[K\u001b[?1h\u001b=\u001b[?2004h"]
[79.833539, "o", "k"]
[79.835133, "o", "\bkubectl logs -f elastic-example-imagenet-worker-1\u001b[48D"]
[80.073586, "o", "\u001b[1C"]
[80.169695, "o", "\u001b[1C"]
[80.385123, "o", "\u001b[1C"]
[80.404855, "o", "\u001b[1C"]
[81.108252, "o", "\u001b[1C"]
[81.422877, "o", "\u001b[1C"]
[81.626703, "o", "\u001b[1C"]
[81.774083, "o", "e                                        \u001b[40D"]
[81.775282, "o", "dit pytorchjobs.kubeflow.org\u001b[28D"]
[83.550427, "o", "\u001b[28C"]
[85.191418, "o", "\u001b[?1l\u001b>\u001b[?2004l\r\r\n\u001bkkubectl\u001b\\"]
[85.40499, "o", "\u001b[?1049h\u001b[?1h\u001b="]
[85.413867, "o", "\u001b[1;35r\u001b[23m\u001b[24m\u001b[0m\u001b[H\u001b[J\u001b[?25l\u001b[35;1H\"/tmp/kubectl-edit-1424300306.yaml\""]
[85.414284, "o", " 62L, 2479C"]
[85.433955, "o", "\u001b[1;1H\u001b[34m# Please edit the object below. Lines beginning with a '#' will be ignored,\r\n# and an empty file will abort the edit. If an error occurs while saving this file will be\r\n# reopened with the relevant failures.\r\n#\u001b[0m\r\n\u001b[36mapiVersion\u001b[0m\u001b[35m:\u001b[0m kubeflow.org/v1\r\n\u001b[36mkind\u001b[0m\u001b[35m:\u001b[0m PyTorchJob\r\n\u001b[36mmetadata\u001b[0m\u001b[35m:\u001b[0m\r\n  \u001b[36mannotations\u001b[0m\u001b[35m:\u001b[0m\r\n    \u001b[36mkubectl.kubernetes.io/last-applied-configuration\u001b[0m\u001b[35m:\u001b[0m |\u001b[10;7H\u001b[35m{\u001b[0m\u001b[31m\"apiVersion\"\u001b[0m\u001b[35m:\u001b[0m\u001b[31m\"kubeflow.org/v1\"\u001b[0m,\u001b[31m\"kind\"\u001b[0m\u001b[35m:\u001b[0m\u001b[31m\"PyTorchJob\"\u001b[0m,\u001b[31m\"metadata\"\u001b[0m\u001b[35m:{\u001b[0m\u001b[31m\"annotations\"\u001b[0m\u001b[35m:{}\u001b[0m,\u001b[31m\"name\"\u001b[0m\u001b[35m:\u001b[0m\u001b[31m\"elastic-example-imagenet\"\u001b[0m,,\u001b[11;1H\u001b[31m\"namespace\"\u001b[0m\u001b[35m:\u001b[0m\u001b[31m\"default\"\u001b[0m\u001b[35m}\u001b[0m,\u001b[31m\"spec\"\u001b[0m\u001b[35m:{\u001b[0m\u001b[31m\"elasticPolicy\"\u001b[0m\u001b[35m:{\u001b[0m\u001b[31m\"maxReplicas\"\u001b[0m\u001b[35m:\u001b[0m2,\u001b[31m\"maxRestarts\"\u001b[0m\u001b[35m:\u001b[0m100,\u001b[31m\"minReplicas\"\u001b[0m\u001b[35m:\u001b[0m1,\u001b[31m\"rdzvBackend\"\u001b[0m\u001b[35m:\u001b[0m\u001b[31m\"c10d\"\u001b[0m\u001b[35m}\u001b[0m,\u001b[31m\"\"\u001b[12;1HpytorchReplicaSpecs\"\u001b[0m\u001b["]
[85.434006, "o", "35m:{\u001b[0m\u001b[31m\"Worker\"\u001b[0m\u001b[35m:{\u001b[0m\u001b[31m\"replicas\"\u001b[0m\u001b[35m:\u001b[0m2,\u001b[31m\"restartPolicy\"\u001b[0m\u001b[35m:\u001b[0m\u001b[31m\"OnFailure\"\u001b[0m,\u001b[31m\"template\"\u001b[0m\u001b[35m:{\u001b[0m\u001b[31m\"spec\"\u001b[0m\u001b[35m:{\u001b[0m\u001b[31m\"containers\"\u001b[0m\u001b[35m:[{\u001b[0m\u001b[31m\"command\"\u001b[0m\u001b[35m:[\u001b[0m\u001b[31m\"pp\u001b[13;1Hython\"\u001b[0m,\u001b[31m\"-m\"\u001b[0m,\u001b[31m\"torch.distributed.run\"\u001b[0m,\u001b[31m\"/workspace/examples/imagenet.py\"\u001b[0m,\u001b[31m\"--arch=resnet18\"\u001b[0m,\u001b[31m\"--epochs=20\"\u001b[0m,\u001b[31m\"--batch-size=32\"\u001b[0m,\u001b[31m\"\"\u001b[14;1H--workers=0\"\u001b[0m,\u001b[31m\"/workspace/data/tiny-imagenet-200\"\u001b[0m\u001b[35m]\u001b[0m,\u001b[31m\"env\"\u001b[0m\u001b[35m:[{\u001b[0m\u001b[31m\"name\"\u001b[0m\u001b[35m:\u001b[0m\u001b[31m\"LOGLEVEL\"\u001b[0m,\u001b[31m\"value\"\u001b[0m\u001b[35m:\u001b[0m\u001b[31m\"DEBUG\"\u001b[0m\u001b[35m}]\u001b[0m,\u001b[31m\"image\"\u001b[0m\u001b[35m:\u001b[0m\u001b[31m\"gaocegege/pytorch--\u001b[15;1Helastic-example-imagenet:1.0.0-sigterm\"\u001b[0m,\u001b[31m\"imagePullPolicy\"\u001b[0m\u001b[35m:\u001b[0m\u001b[31m\"IfNotPresent\"\u001b[0m,\u001b[31m\"name\"\u001b[0m\u001b[35m:\u001b[0m\u001b[31m\"pytorch\"\u001b[0m\u001b[35m}]}}}}}}\u001b[0m\r\n  \u001b[36mcreationTimestamp\u001b[0m\u001b[35m:\u001b[0m \u001b[31m\"2021-11-04T08:22:57Z\"\u001b[0m\r\n  \u001b[36mgeneration"]
[85.436661, "o", "\u001b[0m\u001b[35m:\u001b[0m \u001b[31m1\u001b[0m\r\n  \u001b[36mname\u001b[0m\u001b[35m:\u001b[0m elastic-example-imagenet\r\n  \u001b[36mnamespace\u001b[0m\u001b[35m:\u001b[0m default\r\n  \u001b[36mresourceVersion\u001b[0m\u001b[35m:\u001b[0m \u001b[31m\"764301\"\u001b[0m\r\n  \u001b[36muid\u001b[0m\u001b[35m:\u001b[0m 6694eace-31cf-4ec3-a86b-25d4d5c7c61c\r\n\u001b[36mspec\u001b[0m\u001b[35m:\u001b[0m\r\n  \u001b[36melasticPolicy\u001b[0m\u001b[35m:\u001b[0m\r\n    \u001b[36mmaxReplicas\u001b[0m\u001b[35m:\u001b[0m \u001b[31m2\u001b[0m\r\n    \u001b[36mmaxRestarts\u001b[0m\u001b[35m:\u001b[0m \u001b[31m100\u001b[0m\r\n    \u001b[36mminReplicas\u001b[0m\u001b[35m:\u001b[0m \u001b[31m1\u001b[0m\r\n    \u001b[36mrdzvBackend\u001b[0m\u001b[35m:\u001b[0m c10d\r\n  \u001b[36mpytorchReplicaSpecs\u001b[0m\u001b[35m:\u001b[0m\r\n    \u001b[36mWorker\u001b[0m\u001b[35m:\u001b[0m\u001b[30;7H\u001b[36mreplicas\u001b[0m\u001b[35m:\u001b[0m \u001b[31m2\u001b[0m\u001b[31;7H\u001b[36mrestartPolicy\u001b[0m\u001b[35m:\u001b[0m OnFailure\u001b[32;7H\u001b[36mtemplate\u001b[0m\u001b[35m:\u001b[0m\u001b[33;9H\u001b[36mspec\u001b[0m\u001b[35m:\u001b[0m\u001b[34;11H\u001b[36mcontainers\u001b[0m\u001b[35m:\u001b[0m\u001b[35;104H1,1\u001b[11CTop\u001b[1;1H\u001b[34h\u001b[?25h"]
[86.520601, "o", "\u001b[?25l\u001b[35;94H~@k\u001b[1;1H\u001b[35;94H   \u001b[2;1H\u001b[35;104H2\u001b[2;1H\u001b[34h\u001b[?25h"]
[86.837794, "o", "\u001b[?25l\u001b[35;94H~@k\u001b[2;1H\u001b[35;94H   \u001b[3;1H\u001b[35;104H3\u001b[3;1H\u001b[34h\u001b[?25h"]
[86.871938, "o", "\u001b[?25l\u001b[35;94H~@k\u001b[3;1H\u001b[35;94H   \u001b[4;1H\u001b[35;104H4\u001b[4;1H\u001b[34h\u001b[?25h"]
[86.907366, "o", "\u001b[?25l\u001b[35;94H~@k\u001b[4;1H\u001b[35;94H   \u001b[5;1H\u001b[35;104H5\u001b[5;1H\u001b[34h\u001b[?25h"]
[86.940964, "o", "\u001b[?25l\u001b[35;94H~@k\u001b[5;1H"]
[86.941622, "o", "\u001b[35;94H   \u001b[6;1H\u001b[35;104H6\u001b[6;1H\u001b[34h\u001b[?25h"]
[86.973529, "o", "\u001b[?25l\u001b[35;94H~@k\u001b[6;1H"]
[86.974092, "o", "\u001b[35;94H   \u001b[7;1H\u001b[35;104H7\u001b[7;1H\u001b[34h\u001b[?25h"]
[87.008088, "o", "\u001b[?25l\u001b[35;94H~@k\u001b[7;1H"]
[87.009237, "o", "\u001b[35;94H   \u001b[8;1H\u001b[35;104H8\u001b[8;1H\u001b[34h\u001b[?25h"]
[87.086002, "o", "\u001b[?25l\u001b[35;104H9\u001b[9;1H\u001b[34h\u001b[?25h\u001b[?25l\u001b[35;94H~@k\u001b[9;1H"]
[87.08633, "o", "\u001b[35;94H   \u001b[10;1H\u001b[35;104H10,1\u001b[10;1H\u001b[34h\u001b[?25h"]
[87.110914, "o", "\u001b[?25l\u001b[35;94H~@k\u001b[10;1H\u001b[35;94H   \u001b[16;1H\u001b[35;105H1\u001b[16;1H\u001b[34h\u001b[?25h"]
[87.144258, "o", "\u001b[?25l\u001b[35;94H~@k\u001b[16;1H\u001b[35;94H   \u001b[17;1H\u001b[35;105H2\u001b[17;1H\u001b[34h\u001b[?25h"]
[87.178266, "o", "\u001b[?25l\u001b[35;94H~@k\u001b[17;1H\u001b[35;94H   \u001b[18;1H\u001b[35;105H3\u001b[18;1H\u001b[34h\u001b[?25h"]
[87.211384, "o", "\u001b[?25l\u001b[35;94H~@k\u001b[18;1H\u001b[35;94H   \u001b[19;1H\u001b[35;105H4\u001b[19;1H\u001b[34h\u001b[?25h"]
[87.245001, "o", "\u001b[?25l\u001b[35;94H~@k\u001b[19;1H\u001b[35;94H   \u001b[20;1H\u001b[35;105H5\u001b[20;1H\u001b[34h\u001b[?25h"]
[87.402573, "o", "\u001b[?25l\u001b[35;105H6\u001b[21;1H\u001b[34h\u001b[?25h\u001b[?25l\u001b[35;105H7\u001b[22;1H\u001b[34h\u001b[?25h\u001b[?25l\u001b[35;105H8\u001b[23;1H\u001b[34h\u001b[?25h\u001b[?25l\u001b[35;94H~@k\u001b[23;1H\u001b[35;94H   \u001b[24;1H\u001b[35;105H9\u001b[24;1H\u001b[34h\u001b[?25h"]
[88.163964, "o", "\u001b[?25l\u001b[35;94H~@k\u001b[24;1H\u001b[35;94H   \u001b[25;1H\u001b[35;104H20\u001b[25;1H\u001b[34h\u001b[?25h"]
[88.773744, "o", "\u001b[?25l\u001b[35;94H~@k\u001b[25;1H\u001b[35;94H   \u001b[26;1H\u001b[35;105H1\u001b[26;1H\u001b[34h\u001b[?25h"]
[88.981839, "o", "\u001b[?25l\u001b[35;94H~@k\u001b[26;1H\u001b[35;94H   \u001b[27;1H\u001b[35;105H2\u001b[27;1H\u001b[34h\u001b[?25h"]
[89.193728, "o", "\u001b[?25l\u001b[35;94H~@k\u001b[27;1H"]
[89.193899, "o", "\u001b[35;94H   \u001b[28;1H\u001b[35;105H3\u001b[28;1H\u001b[34h\u001b[?25h"]
[89.385867, "o", "\u001b[?25l\u001b[35;94H~@k\u001b[28;1H\u001b[35;94H   \u001b[29;1H\u001b[35;105H4\u001b[29;1H\u001b[34h\u001b[?25h"]
[89.937999, "o", "\u001b[?25l\u001b[35;94H~@k\u001b[29;1H\u001b[35;94H   \u001b[29;2H\u001b[35;107H2\u001b[29;2H\u001b[34h\u001b[?25h"]
[90.387738, "o", "\u001b[?25l\u001b[35;94H~@k\u001b[29;2H\u001b[35;94H   \u001b[29;3H\u001b[35;107H3\u001b[29;3H\u001b[34h\u001b[?25h"]
[90.821879, "o", "\u001b[?25l\u001b[35;94H~@k\u001b[29;3H\u001b[35;94H   \u001b[29;4H\u001b[35;107H4\u001b[29;4H\u001b[34h\u001b[?25h"]
[90.839524, "o", "\u001b[?25l\u001b[35;94H~@k\u001b[29;4H\u001b[35;94H   \u001b[29;5H\u001b[35;107H5\u001b[29;5H\u001b[34h\u001b[?25h"]
[91.271764, "o", "\u001b[?25l\u001b[35;107H6\u001b[29;6H\u001b[34h\u001b[?25h\u001b[?25l\u001b[35;107H7\u001b[29;7H\u001b[34h\u001b[?25h\u001b[?25l\u001b[35;107H8\u001b[29;8H\u001b[34h\u001b[?25h\u001b[?25l\u001b[35;107H9\u001b[29;9H\u001b[34h\u001b[?25h\u001b[?25l\u001b[35;107H10\u001b[29;10H\u001b[34h\u001b[?25h\u001b[?25l\u001b[35;108H1\u001b[29;11H\u001b[34h\u001b[?25h\u001b[?25l\u001b[35;94H~@k\u001b[29;11H\u001b[35;94H   \u001b[29;11H\u001b[1;34r\u001b[34;1H\r\n\u001b[1;35r\u001b[34;11H\u001b[33m- \u001b[0m\u001b[36mcommand\u001b[0m\u001b[35m:\u001b[0m\u001b[35;1H\u001b[K\u001b[35;104H25,11\u001b[10C3%\u001b[29;11H\u001b[34h\u001b[?25h"]
[91.351707, "o", "\u001b[?25l\u001b[35;94H~@k\u001b[29;11H\u001b[35;94H   \u001b[29;12H\u001b[35;108H2\u001b[29;12H\u001b[34h\u001b[?25h"]
[91.846858, "o", "\u001b[?25l\u001b[35;108H3\u001b[29;13H\u001b[34h\u001b[?25h\u001b[?25l\u001b[35;108H4\u001b[29;14H\u001b[34h\u001b[?25h\u001b[?25l\u001b[35;94H~@k\u001b[29;14H"]
[91.851111, "o", "\u001b[35;94H   \u001b[29;15H\u001b[35;108H5\u001b[29;15H\u001b[34h\u001b[?25h"]
[91.868836, "o", "\u001b[?25l\u001b[35;94H~@k\u001b[29;15H\u001b[35;94H   \u001b[29;16H\u001b[35;108H6\u001b[29;16H\u001b[34h\u001b[?25h"]
[91.904901, "o", "\u001b[?25l\u001b[35;94H~@k\u001b[29;16H\u001b[35;94H   \u001b[29;17H\u001b[35;108H7\u001b[29;17H\u001b[34h\u001b[?25h"]
[91.947856, "o", "\u001b[?25l\u001b[35;94H~@k\u001b[29;17H\u0007\u001b[35;94H   \u001b[29;17H\u001b[34h\u001b[?25h"]
[91.97555, "o", "\u001b[?25l\u001b[35;94H~@k\u001b[29;17H\u001b[35;94H   \u001b[29;17H\u001b[34h\u001b[?25h"]
[92.011678, "o", "\u001b[?25l\u001b[35;94H~@k\u001b[29;17H\u001b[35;94H   \u001b[29;17H\u001b[34h\u001b[?25h"]
[92.036223, "o", "\u001b[?25l\u001b[35;94H~@k\u001b[29;17H\u001b[35;94H   \u001b[29;17H\u001b[34h\u001b[?25h"]
[92.077708, "o", "\u001b[?25l\u001b[35;94H~@k\u001b[29;17H\u001b[35;94H   \u001b[29;17H\u001b[34h\u001b[?25h"]
[93.587075, "o", "\u001b[?25l\u001b[35;94Ha\u001b[29;17H\u001b[35;94H \u001b[29;18H\u001b[35;1H\u001b[1m-- INSERT --\u001b[0m\u001b[35;104H\u001b[K\u001b[35;104H25,18\u001b[10C3%\u001b[29;18H\u001b[34h\u001b[?25h"]
[93.995589, "o", "\u001b[?25l\u001b[29;17H\u001b[K\u001b[35;108H7\u001b[29;17H\u001b[34h\u001b[?25h"]
[95.226305, "o", "\u001b[?25l\u001b[31m1\u001b[0m\u001b[35;108H8\u001b[29;18H\u001b[34h\u001b[?25h"]
[96.662099, "o", "\u001b[35;1H\u001b[K\u001b[29;17H\u001b[?25l\u001b[35;94H^[\u001b[29;17H"]
[96.762442, "o", "\u001b[35;94H  \u001b[29;18H\u001b[35;104H25,17\u001b[10C3%\u001b[29;17H\u001b[34h\u001b[?25h"]
[97.947581, "o", "\u001b[?25l\u001b[35;94H~@k\u001b[29;17H\u001b[35;94H   \u001b[29;17H\u001b[1;34r\u001b[34;1H\r\n\u001b[1;35r\u001b[34;13H\u001b[33m- \u001b[0mpython\u001b[35;104H\u001b[K\u001b[35;104H26,17\u001b[10C6%\u001b[29;17H\u001b[34h\u001b[?25h"]
[98.209769, "o", "\u001b[?25l\u001b[35;94H~@k\u001b[29;17H\u001b[35;94H   \u001b[29;15H\u001b[1;34r\u001b[34;1H\r\n\u001b[1;35r\u001b[34;13H\u001b[33m- \u001b[0m-m\u001b[35;104H\u001b[K\u001b[35;104H27,15\u001b[10C9%\u001b[29;15H\u001b[34h\u001b[?25h"]
[98.605878, "o", "\u001b[?25l\u001b[35;94H~@k\u001b[29;15H\u001b[35;94H   \u001b[29;13H\u001b[1;34r\u001b[34;1H\r\n\u001b[1;35r\u001b[34;13H\u001b[33m- \u001b[0mtorch.distributed.run\u001b[35;104H\u001b[K\u001b[35;104H28,13\u001b[9C12%\u001b[29;13H\u001b[34h\u001b[?25h"]
[99.435454, "o", "\u001b[?25l\u001b[35;94H:\u001b[29;13H\u001b[35;94H\u001b[K\u001b[35;1H:\u001b[34h\u001b[?25h"]
[99.831324, "o", "w"]
[99.90792, "o", "q"]
[100.507414, "o", "\r\u001b[?25l\"/tmp/kubectl-edit-1424300306.yaml\""]
[100.511156, "o", " 62L, 2479C written\r\r\r\n\u001b[?1l\u001b>\u001b[34h\u001b[?25h\u001b[?1049l"]
[100.668565, "o", "pytorchjob.kubeflow.org/elastic-example-imagenet edited\r\n"]
[100.676874, "o", "\u001b[1m\u001b[3m%\u001b[23m\u001b[1m\u001b[0m                                                                                                                        \r \r\u001bk..ning-operator\u001b\\"]
[100.704714, "o", "\r\u001b[0m\u001b[23m\u001b[24m\u001b[J(base) \u001b[01;32m➜  \u001b[36mtraining-operator\u001b[00m \u001b[01;34mgit:(\u001b[31mpytorchelastic\u001b[34m) \u001b[33m✗\u001b[00m "]
[100.705227, "o", "\u001b[K\u001b[?1h\u001b="]
[100.70554, "o", "\u001b[?2004h"]
[101.819121, "o", "kubectl edit pytorchjobs.kubeflow.org"]
[103.056667, "o", "\u001b[29Dlogs -f elastic-example-imagenet-worker-1"]
[103.624558, "o", "\b0"]
[104.445381, "o", "\u001b[36D\u001b[3P\u001b[33C   \b\b\b"]
[104.866697, "o", "\b1"]
[105.267884, "o", "\u001b[38Dget pods                              \u001b[30D"]
[106.962359, "o", "\u001b[16D"]
[107.61489, "o", "wkubectl get pods\u001b[16D"]
[107.643677, "o", "\bwakubectl get pods\u001b[16D"]
[107.721278, "o", "tkubectl get pods\u001b[16D"]
[107.855319, "o", "ckubectl get pods\u001b[16D"]
[107.957297, "o", "hkubectl get pods\u001b[16D"]
[108.081249, "o", " kubectl get pods\u001b[16D"]
[108.538276, "o", "\u001b[?1l\u001b>\u001b[?2004l\r\r\n\u001bkwatch\u001b\\"]
[108.627548, "o", "\u001b(B\u001b)0\u001b[?1049h\u001b[1;35r\u001b[m\u000f\u001b[4l\u001b[H\u001b[J"]
[108.6276, "o", "Every 2.0s: kubectl get pods\u001b[1;91Hcegao: Thu Nov  4 16:24:32 2021\u001b[3;1HNAME\u001b[3;37HREADY   STATUS\u001b[3;59HRESTARTS   AGE\r\u001b[4delastic-example-imagenet-worker-0   1/1     Running\u001b[59G0\u001b[4;70H95s\r\u001b[5delastic-example-imagenet-worker-1   1/1     Terminating   1\u001b[5;70H95s\u001b[35;121H"]
[110.749894, "o", "\u001b[1;116H4\u001b[4;71H"]
[110.750876, "o", "8\u001b[5d\b8\u001b[35;121H"]
[112.870312, "o", "\u001b[1;116H"]
[112.871445, "o", "7\u001b[4;70H100s\u001b[5;70H100s\u001b[35;121H"]
[115.033261, "o", "\u001b[1;116H9\u001b[4;72H2\u001b[5d\b2\u001b[35;121H"]
[117.224573, "o", "\u001b[1;115H41\u001b[4;72H4\u001b[5d\b4\u001b[35;121H"]
[119.301107, "o", "\u001b[1;116H3\u001b[4;72H6\u001b[5d\b6\u001b[35;121H"]
[121.414687, "o", "\u001b[1;116H"]
[121.415593, "o", "5\u001b[4;72H"]
[121.415781, "o", "8\u001b[5d\b"]
[121.415942, "o", "8\u001b[35;121H"]
[123.532407, "o", "\u001b[1;116H7\u001b[4;71H10\u001b[5d\b\b10\u001b[35;121H"]
[125.657, "o", "\u001b[1;116H9\u001b[4;72H3\u001b[5d\b3\u001b[35;121H"]
[127.776957, "o", "\u001b[1;115H52\u001b[4;72H5\u001b[5d\b5\u001b[35;121H"]
[129.865449, "o", "\u001b[1;116H4\u001b[4;72H7\u001b[5d\b"]
[129.865901, "o", "7\u001b[35;121H"]
[131.909378, "o", "\u001b[1;116H6\u001b[4;72H9\u001b[5;37H0\u001b[5;72H9\u001b[35;121H"]
[133.958348, "o", "\u001b[1;116H8\u001b[4;70H2m1\u001b[5;70H2m1\u001b[35;121H"]
[136.010428, "o", "\u001b[1;113H5:00\u001b[4;72H3\u001b[5d\b3\u001b[35;121H"]
[138.062822, "o", "\u001b[1;116H2\u001b[4;72H5\u001b[5d\b5\u001b[35;121H"]
[140.113205, "o", "\u001b[1;116H4\u001b[4;72H7\u001b[5d\b7\u001b[35;121H"]
[142.162855, "o", "\u001b[1;116H6\u001b[4;72H9\u001b[5d\b9\u001b[35;121H"]
[144.214741, "o", "\r\u001b[5d"]
[144.214858, "o", "\u001b[J\u001b[1;116H8\u001b[3;51H\u001b[4P\u001b[4;55H0    \u001b[66G2m11s\u001b[K\u001b[35;121H"]
[146.278987, "o", "\u001b[1;115H10\u001b[4;69H3\u001b[35;121H"]
[148.332682, "o", "\u001b[1;116H2\u001b[4;69H5\u001b[35;121H"]
[148.867835, "o", "\u001b[35;1H\u001b[?1049l\r\u001b[?1l\u001b>"]
[148.86816, "o", "\u001b[1m\u001b[3m%\u001b[23m\u001b[1m\u001b[0m                                                                                                                        \r \r"]
[148.868306, "o", "\u001bk..ning-operator\u001b\\"]
[148.899058, "o", "\r\u001b[0m\u001b[23m\u001b[24m\u001b[J(base) \u001b[01;32m➜  \u001b[36mtraining-operator\u001b[00m \u001b[01;34mgit:(\u001b[31mpytorchelastic\u001b[34m) \u001b[33m✗\u001b[00m \u001b[K"]
[148.899183, "o", "\u001b[?1h\u001b=\u001b[?2004h"]
[149.497957, "o", "k"]
[149.499207, "o", "\bkubectl edit pytorchjobs.kubeflow.org\u001b[36D"]
[149.597041, "o", "\u001b[1C"]
[149.804709, "o", "\u001b[1C"]
[150.043916, "o", "\u001b[1C"]
[150.280674, "o", "\u001b[1C"]
[150.623546, "o", "\u001b[1C"]
[150.724804, "o", "\u001b[1C"]
[150.929878, "o", "\u001b[1C"]
[151.059685, "o", "g                            \u001b[28D"]
[151.060991, "o", "et pods\b\b\b\b\b\b\b"]
[151.178373, "o", "\u001b[1C"]
[151.277717, "o", "\u001b[1C"]
[151.377973, "o", "\u001b[1C"]
[151.748746, "o", "\u001b[1C"]
[151.834778, "o", "\u001b[1C"]
[151.99094, "o", "\u001b[1C"]
[152.040012, "o", "\u001b[1C"]
[152.363873, "o", "\u001b[?1l\u001b>"]
[152.364014, "o", "\u001b[?2004l\r\r\n"]
[152.364385, "o", "\u001bkkubectl\u001b\\"]
[152.410743, "o", "NAME                                READY   STATUS    RESTARTS   AGE\r\nelastic-example-imagenet-worker-0   1/1     Running   0          2m19s\r\n"]
[152.412699, "o", "\u001b[1m\u001b[3m%\u001b[23m\u001b[1m\u001b[0m                                                                                                                        \r \r"]
[152.412809, "o", "\u001bk..ning-operator\u001b\\"]
[152.438262, "o", "\r\u001b[0m\u001b[23m\u001b[24m\u001b[J(base) \u001b[01;32m➜  \u001b[36mtraining-operator\u001b[00m \u001b[01;34mgit:(\u001b[31mpytorchelastic\u001b[34m) \u001b[33m✗\u001b[00m \u001b[K"]
[152.438353, "o", "\u001b[?1h\u001b=\u001b[?2004h"]
[154.416951, "o", "k"]
[154.418142, "o", "\bkubectl get pods\u001b[15D"]
[154.568229, "o", "\u001b[1C"]
[154.719394, "o", "\u001b[1C"]
[154.819627, "o", "\u001b[1C"]
[154.895241, "o", "\u001b[1C"]
[155.200447, "o", "\u001b[1C"]
[155.371724, "o", "\u001b[1C"]
[155.544933, "o", "\u001b[1C"]
[155.74384, "o", "\u001b[1C"]
[155.790771, "o", "\u001b[1C"]
[155.881701, "o", "\u001b[1C"]
[155.959324, "o", "\u001b[1C"]
[156.301573, "o", "t   \b\b\b"]
[156.418967, "o", "f"]
[156.652423, "o", "j"]
[156.727337, "o", "o"]
[156.97158, "o", "b"]
[157.282961, "o", "\b \b"]
[157.380393, "o", "\b \b"]
[157.482533, "o", "\b \b"]
[157.585205, "o", "\b \b"]
[157.749224, "o", "\b \b"]
[157.750259, "o", "pods\b\b\b\b"]
[158.017742, "o", "\u001b[1C"]
[158.30366, "o", "y  \b\b"]
[158.438385, "o", "t"]
[158.713083, "o", "o"]
[158.714004, "o", "r"]
[159.020193, "o", "c"]
[159.039384, "o", "h"]
[159.328635, "o", "j"]
[159.329545, "o", "o"]
[159.531322, "o", "b"]
[159.553688, "o", "s"]
[159.946407, "o", " "]
[159.947412, "o", "-"]
[160.250118, "o", "o"]
[160.413402, "o", " "]
[160.555915, "o", "y"]
[160.673587, "o", "a"]
[160.851488, "o", "m"]
[160.874312, "o", "l"]
[161.373335, "o", "\u001b[?1l\u001b>"]
[161.373471, "o", "\u001b[?2004l\r\r\n"]
[161.373851, "o", "\u001bkkubectl\u001b\\"]
[161.41495, "o", "apiVersion: v1\r\nitems:\r\n- apiVersion: kubeflow.org/v1\r\n  kind: PyTorchJob\r\n  metadata:\r\n    annotations:\r\n      kubectl.kubernetes.io/last-applied-configuration: |\r\n        {\"apiVersion\":\"kubeflow.org/v1\",\"kind\":\"PyTorchJob\",\"metadata\":{\"annotations\":{},\"name\":\"elastic-example-imagenet\",\"namespace\":\"default\"},\"spec\":{\"elasticPolicy\":{\"maxReplicas\":2,\"maxRestarts\":100,\"minReplicas\":1,\"rdzvBackend\":\"c10d\"},\"pytorchReplicaSpecs\":{\"Worker\":{\"replicas\":2,\"restartPolicy\":\"OnFailure\",\"template\":{\"spec\":{\"containers\":[{\"command\":[\"python\",\"-m\",\"torch.distributed.run\",\"/workspace/examples/imagenet.py\",\"--arch=resnet18\",\"--epochs=20\",\"--batch-size=32\",\"--workers=0\",\"/workspace/data/tiny-imagenet-200\"],\"env\":[{\"name\":\"LOGLEVEL\",\"value\":\"DEBUG\"}],\"image\":\"gaocegege/pytorch-elastic-example-imagenet:1.0.0-sigterm\",\"imagePullPolicy\":\"IfNotPresent\",\"name\":\"pytorch\"}]}}}}}}\r\n    creationTimestamp: \"2021-11-04T08:22:57Z\"\r\n    generation: 2\r\n    name: elastic-example-imagenet\r\n    namespace: default\r\n    resourceVersion: \"76454"]
[161.41511, "o", "3\"\r\n    uid: 6694eace-31cf-4ec3-a86b-25d4d5c7c61c\r\n  spec:\r\n    elasticPolicy:\r\n      maxReplicas: 2\r\n      maxRestarts: 100\r\n      minReplicas: 1\r\n      rdzvBackend: c10d\r\n    pytorchReplicaSpecs:\r\n      Worker:\r\n        replicas: 1\r\n        restartPolicy: OnFailure\r\n        template:\r\n          spec:\r\n            containers:\r\n            - command:\r\n              - python\r\n              - -m\r\n              - torch.distributed.run\r\n              - /workspace/examples/imagenet.py\r\n              - --arch=resnet18\r\n              - --epochs=20\r\n              - --batch-size=32\r\n              - --workers=0\r\n              - /workspace/data/tiny-imagenet-200\r\n              env:\r\n              - name: LOGLEVEL\r\n                value: DEBUG\r\n              image: gaocegege/pytorch-elastic-example-imagenet:1.0.0-sigterm\r\n              imagePullPolicy: IfNotPresent\r\n              name: pytorch\r\n  status:\r\n    conditions:\r\n    - lastTransitionTime: \"2021-11-04T08:22:57Z\"\r\n      lastUpdateTime: \"2021-11-04T08:22:57Z\"\r\n    "]
[161.415155, "o", "  message: PyTorchJob elastic-example-imagenet is created.\r\n      reason: PyTorchJobCreated\r\n      status: \"True\"\r\n      type: Created\r\n    - lastTransitionTime: \"2021-11-04T08:22:57Z\"\r\n      lastUpdateTime: \"2021-11-04T08:22:57Z\"\r\n      message: PyTorchJob elastic-example-imagenet is running.\r\n      reason: JobRunning\r\n      status: \"True\"\r\n      type: Running\r\n    replicaStatuses:\r\n      Worker:\r\n        active: 1\r\nkind: List\r\nmetadata:\r\n  resourceVersion: \"\"\r\n  selfLink: \"\"\r\n"]
[161.416756, "o", "\u001b[1m\u001b[3m%\u001b[23m\u001b[1m\u001b[0m                                                                                                                        \r \r"]
[161.416853, "o", "\u001bk..ning-operator\u001b\\"]
[161.444624, "o", "\r\u001b[0m\u001b[23m\u001b[24m\u001b[J(base) \u001b[01;32m➜  \u001b[36mtraining-operator\u001b[00m \u001b[01;34mgit:(\u001b[31mpytorchelastic\u001b[34m) \u001b[33m✗\u001b[00m \u001b[K"]
[161.444744, "o", "\u001b[?1h\u001b=\u001b[?2004h"]
[173.76646, "o", "k"]
[173.767556, "o", "\bkubectl get pytorchjobs -o yaml\u001b[30D"]
[173.90217, "o", "\u001b[1C"]
[174.072941, "o", "\u001b[1C"]
[174.154296, "o", "\u001b[1C"]
[174.994218, "o", "\u001b[1C"]
[175.440088, "o", "\u001b[1C"]
[175.609402, "o", "\u001b[1C"]
[175.64258, "o", "\u001b[1C"]
[175.836254, "o", "l                      \u001b[22D"]
[175.837584, "o", "ogs -f elastic-example-imagenet-worker-1\u001b[40D"]
[175.976845, "o", "\u001b[1C"]
[176.175624, "o", "\u001b[1C"]
[176.340069, "o", "\u001b[1C"]
[176.364006, "o", "\u001b[1C"]
[176.83979, "o", "\u001b[36C"]
[177.116896, "o", "\b \b"]
[177.118118, "o", "1\b"]
[177.781369, "o", "0"]
[179.094635, "o", "\u001b[?1l\u001b>"]
[179.095052, "o", "\u001b[?2004l\r\r\n"]
[179.096452, "o", "\u001bkkubectl\u001b\\"]
[179.183635, "o", "INFO:torch.distributed.launcher.api:Starting elastic_operator with launch configs:\r\n  entrypoint       : /workspace/examples/imagenet.py\r\n"]
[179.184019, "o", "  min_nodes        : 1\r\n  max_nodes        : 2\r\n  nproc_per_node   : 1\r\n  run_id           : none\r\n"]
[179.184323, "o", "  rdzv_backend     : c10d\r\n  rdzv_endpoint    : elastic-example-imagenet-worker-0:23456\r\n  rdzv_configs     : {'timeout': 900}\r\n  max_restarts     : 100\r\n  monitor_interval : 5\r\n  log_dir          : None"]
[179.184337, "o", "\r\n  metrics_cfg      : {}\r\n\r\n"]
[179.185417, "o", "INFO:torch.distributed.elastic.agent.server.local_elastic_agent:log directory set to: /tmp/torchelastic_m8me90uk/none_qs2bblei\r\n"]
[179.18551, "o", "INFO:torch.distributed.elastic.agent.server.api:[default] starting workers for entrypoint: python\r\nINFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous'ing worker group\r\n"]
[179.185852, "o", "INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous complete for workers. Result:\r\n"]
[179.185958, "o", "  restart_count=0\r\n  master_addr=elastic-example-imagenet-worker-0\r\n  master_port=49951\r\n  group_rank=0\r\n  group_world_size=2\r\n  local_ranks=[0]\r\n  role_ranks=[0]\r\n  global_ranks=[0]\r\n"]
[179.186004, "o", "  role_world_sizes=[2]\r\n  global_world_sizes=[2]\r\n\r\n"]
[179.186017, "o", "INFO:torch.distributed.elastic.agent.server.api:[default] Starting worker group\r\nINFO:torch.distributed.elastic.multiprocessing:Setting worker0 reply file to: /tmp/torchelastic_m8me90uk/none_qs2bblei/attempt_0/0/error.json\r\n"]
[179.188315, "o", "INFO:torch.distributed.elastic.agent.server.api:[default] Detected 1 new nodes from group_rank=0; will restart worker group\r\nINFO:torch.distributed.elastic.agent.server.api:[default] Stopping worker group\r\nWARNING:torch.distributed.elastic.multiprocessing.api:Sending process 12 closing signal SIGTERM\r\n"]
[179.188445, "o", "INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous'ing worker group\r\nINFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous complete for workers. Result:\r\n  restart_count=0\r\n  master_addr=elastic-example-imagenet-worker-0\r\n  master_port=35023\r\n  group_rank=0\r\n  group_world_size=2\r\n  local_ranks=[0]\r\n  role_ranks=[0]\r\n  global_ranks=[0]\r\n  role_world_sizes=[2]\r\n  global_world_sizes=[2]\r\n\r\nINFO:torch.distributed.elastic.agent.server.api:[default] Starting worker group\r\nINFO:torch.distributed.elastic.multiprocessing:Setting worker0 reply file to: /tmp/torchelastic_m8me90uk/none_qs2bblei/attempt_0/0/error.json\r\n"]
[179.188528, "o", "INFO:torch.distributed.elastic.agent.server.api:[default] Detected 1 new nodes from group_rank=0; will restart worker group\r\n"]
[179.188723, "o", "INFO:torch.distributed.elastic.agent.server.api:[default] Stopping worker group\r\n"]
[179.189099, "o", "WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 17 closing signal SIGTERM\r\nINFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous'ing worker group\r\nINFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous complete for workers. Result:\r\n  restart_count=0\r\n"]
[179.189455, "o", "  master_addr=elastic-example-imagenet-worker-0\r\n  master_port=51241\r\n  group_rank=0\r\n  group_world_size=2\r\n"]
[179.189516, "o", "  local_ranks=[0]\r\n  role_ranks=[0]\r\n"]
[179.189657, "o", "  global_ranks=[0]\r\n"]
[179.189702, "o", "  role_world_sizes=[2]\r\n  global_world_sizes=[2]\r\n"]
[179.189723, "o", "\r\n"]
[179.190168, "o", "INFO:torch.distributed.elastic.agent.server.api:[default] Starting worker group\r\n"]
[179.191399, "o", "INFO:torch.distributed.elastic.multiprocessing:Setting worker0 reply file to: /tmp/torchelastic_m8me90uk/none_qs2bblei/attempt_0/0/error.json\r\nINFO:torch.distributed.elastic.agent.server.api:[default] Detected 1 new nodes from group_rank=0; will restart worker group\r\nINFO:torch.distributed.elastic.agent.server.api:[default] Stopping worker group\r\nWARNING:torch.distributed.elastic.multiprocessing.api:Sending process 22 closing signal SIGTERM\r\nINFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous'ing worker group\r\n"]
[179.19151, "o", "INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous complete for workers. Result:\r\n  restart_count=0\r\n  master_addr=elastic-example-imagenet-worker-0\r\n  master_port=52111\r\n  group_rank=0\r\n  group_world_size=2\r\n  local_ranks=[0]\r\n  role_ranks=[0]\r\n  global_ranks=[0]\r\n  role_world_sizes=[2]\r\n  global_world_sizes=[2]\r\n\r\n"]
[179.191586, "o", "INFO:torch.distributed.elastic.agent.server.api:[default] Starting worker group\r\nINFO:torch.distributed.elastic.multiprocessing:Setting worker0 reply file to: /tmp/torchelastic_m8me90uk/none_qs2bblei/attempt_0/0/error.json\r\n=> creating model: resnet18\r\n=> no workers have checkpoints, starting from epoch 0\r\n=> start_epoch: 0, best_acc1: 0\r\nEpoch: [0][   0/1563]"]
[179.191597, "o", "\tTime  2.085 ( 2.085)\tData  0.097 ( 0.097)\tLoss 7.0790e+00 (7.0790e+00)\tAcc@1   0.00 (  0.00)\tAcc@5   0.00 (  0.00)\r\n"]
[179.191603, "o", "Epoch: [0][  10/1563]\tTime  1.968 ( 1.946)\tData  0.064 ( 0.051)\tLoss 6.6233e+00 (6.7385e+00)\tAcc@1   0.00 (  0.28)\tAcc@5   0.00 (  1.99)\r\n"]
[179.19192, "o", "Epoch: [0][  20/1563]\tTime  1.877 ( 1.924)\tData  0.044 ( 0.047)\tLoss 5.8902e+00 (6.5127e+00)\tAcc@1   3.12 (  0.30)\tAcc@5   6.25 (  2.98)\r\nEpoch: [0][  30/1563]\tTime  1.936 ( 1.925)\tData  0.050 ( 0.045)\tLoss 5.8298e+00 (6.3235e+00)\tAcc@1   0.00 (  0.60)\tAcc@5   0.00 (  4.03)\r\nEpoch: [0][  40/1563]\tTime  1.911 ( 1.925)\tData  0.037 ( 0.046)\tLoss 5.4624e+00 (6.1743e+00)\tAcc@1   6.25 (  0.76)\tAcc@5   6.25 (  4.57)\r\nTraceback (most recent call last):\r\n  File \"/workspace/examples/imagenet.py\", line 580, in <module>\r\n    main()\r\n  File \"/opt/conda/lib/python3.7/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 345, in wrapper\r\n    return f(*args, **kwargs)\r\n  File \"/workspace/examples/imagenet.py\", line 181, in main\r\n    train(train_loader, model, criterion, optimizer, epoch, print_freq)\r\n  File \"/workspace/examples/imagenet.py\", line 442, in train\r\n    output = model(images)\r\n"]
[179.192007, "o", "  File \"/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\r\n    return forward_call(*input, **kwargs)\r\n"]
[179.192127, "o", "  File \"/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/distributed.py\", line 878, in forward\r\n"]
[179.192272, "o", "    self._sync_params()"]
[179.192388, "o", "\r\n"]
[179.192451, "o", "  File \"/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/distributed.py\", line 1382, in _sync_params\r\n    authoritative_rank,\r\n"]
[179.192733, "o", "  File \"/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/distributed.py\", line 1335, in _distributed_broadcast_coalesced\r\n    self.process_group, tensors, buffer_size, authoritative_rank\r\n"]
[179.192802, "o", "RuntimeError: [/opt/conda/conda-bld/pytorch_1634272168290/work/third_party/gloo/gloo/transport/tcp/pair.cc:589] Read error [10.244.0.47]:57480: Connection reset by peer\r\nDEBUG:torch.distributed.elastic.multiprocessing.errors:User process failed with error data: {\r\n"]
[179.193093, "o", "  \"message\": {\r\n    \"message\": \"RuntimeError: [/opt/conda/conda-bld/pytorch_1634272168290/work/third_party/gloo/gloo/transport/tcp/pair.cc:589] Read error [10.244.0.47]:57480: Connection reset by peer\",\r\n    \"extraInfo\": {\r\n"]
[179.19326, "o", "      \"py_callstack\": \"Traceback (most recent call last):\\n  File \\\"/opt/conda/lib/python3.7/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\\\", line 345, in wrapper\\n    return f(*args, **kwargs)\\n  File \\\"/workspace/examples/imagenet.py\\\", line 181, in main\\n    train(train_loader, model, criterion, optimizer, epoch, print_freq)\\n  File \\\"/workspace/examples/imagenet.py\\\", line 442, in train\\n    output = model(images)\\n  File \\\"/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\\\", line 1102, in _call_impl\\n    return forward_call(*input, **kwargs)\\n  File \\\"/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/distributed.py\\\", line 878, in forward\\n    self._sync_params()\\n  File \\\"/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/distributed.py\\\", line 1382, in _sync_params\\n    authoritative_rank,\\n  File \\\"/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/distributed.py\\\", line 1335, in _distributed_broadcast_coalesced\\n    self.process_group, t"]
[179.193463, "o", "ensors, buffer_size, authoritative_rank\\nRuntimeError: [/opt/conda/conda-bld/pytorch_1634272168290/work/third_party/gloo/gloo/transport/tcp/pair.cc:589] Read error [10.244.0.47]:57480: Connection reset by peer\\n\",\r\n"]
[179.194629, "o", "      \"timestamp\": \"1636014295\"\r\n    }\r\n  }\r\n}\r\n"]
[179.19469, "o", "ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 27) of binary: /opt/conda/bin/python\r\nINFO:torch.distributed.elastic.agent.server.api:[default] Worker group FAILED. 100/100 attempts left; will restart worker group\r\nINFO:torch.distributed.elastic.agent.server.api:[default] Stopping worker group\r\nINFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous'ing worker group\r\n"]
[179.195362, "o", "INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous complete for workers. Result:\r\n  restart_count=1\r\n  master_addr=elastic-example-imagenet-worker-0\r\n"]
[179.1955, "o", "  master_port=48739\r\n  group_rank=0\r\n  group_world_size=1\r\n  local_ranks=[0]\r\n  role_ranks=[0]\r\n  global_ranks=[0]\r\n"]
[179.195597, "o", "  role_world_sizes=[1]\r\n  global_world_sizes=[1]\r\n\r\n"]
[179.195652, "o", "INFO:torch.distributed.elastic.agent.server.api:[default] Starting worker group\r\n"]
[179.195876, "o", "INFO:torch.distributed.elastic.multiprocessing:Setting worker0 reply file to: /tmp/torchelastic_m8me90uk/none_qs2bblei/attempt_1/0/error.json\r\n=> creating model: resnet18"]
[179.19592, "o", "\r\n=> no workers have checkpoints, starting from epoch 0\r\n=> start_epoch: 0, best_acc1: 0\r\n"]
[179.196397, "o", "Epoch: [0][   0/3125]"]
[179.196548, "o", "\tTime  1.112 ( 1.112)\tData  0.041 ( 0.041)\tLoss 7.1575e+00 (7.1575e+00)\tAcc@1   0.00 (  0.00)\tAcc@5   0.00 (  0.00)\r\n"]
[187.997193, "o", "Epoch: [0][  10/3125]\tTime  1.030 ( 1.092)\tData  0.030 ( 0.032)\tLoss 7.0225e+00 (7.1951e+00)\tAcc@1   3.12 (  0.85)\tAcc@5   3.12 (  2.84)\r\n"]
[190.047451, "o", "^C"]
[190.04931, "o", "\r\n"]
[190.049468, "o", "\u001b[1m\u001b[3m%\u001b[23m\u001b[1m\u001b[0m                                                                                                                        \r \r\u001bk..ning-operator\u001b\\"]
[190.095215, "o", "\r\u001b[0m\u001b[23m\u001b[24m\u001b[J(base) \u001b[01;31m➜  \u001b[36mtraining-operator\u001b[00m \u001b[01;34mgit:(\u001b[31mpytorchelastic\u001b[34m) \u001b[33m✗\u001b[00m \u001b[K\u001b[?1h\u001b="]
[190.095702, "o", "\u001b[?2004h"]
[191.075776, "o", "#"]
[191.186864, "o", "\b# "]
[191.434169, "o", "N"]
[191.687392, "o", "o"]
[191.688457, "o", "w"]
[191.734548, "o", " "]
[192.333444, "o", "w"]
[192.608397, "o", "e"]
[192.609445, "o", " "]
[192.770101, "o", "s"]
[193.018222, "o", "c"]
[193.021214, "o", "a"]
[193.110094, "o", "l"]
[193.837661, "o", "e"]
[193.840032, "o", " "]
[194.216974, "o", "u"]
[194.263627, "o", "p"]
[194.456203, "o", " "]
[194.891393, "o", "t"]
[195.06918, "o", "h"]
[195.121238, "o", "e"]
[195.169893, "o", " "]
[196.007893, "o", "j"]
[196.054873, "o", "o"]
[196.25592, "o", "b"]
[196.324023, "o", " "]
[196.501483, "o", "t"]
[196.546857, "o", "o"]
[196.658714, "o", " "]
[196.913949, "o", "2"]
[197.010529, "o", " "]
[197.646634, "o", "\b"]
[198.239975, "o", "\u001b[?1l\u001b>"]
[198.240019, "o", "\u001b[?2004l\r\r\n"]
[198.240728, "o", "\u001b[1m\u001b[3m%\u001b[23m\u001b[1m\u001b[0m                                                                                                                        \r \r"]
[198.241155, "o", "\u001bk..ning-operator\u001b\\"]
[198.291976, "o", "\r\u001b[0m\u001b[23m\u001b[24m\u001b[J(base) \u001b[01;31m➜  \u001b[36mtraining-operator\u001b[00m \u001b[01;34mgit:(\u001b[31mpytorchelastic\u001b[34m) \u001b[33m✗\u001b[00m \u001b[K"]
[198.2922, "o", "\u001b[?1h\u001b="]
[198.292632, "o", "\u001b[?2004h"]
[200.345223, "o", "k"]
[200.348948, "o", "\bkubectl logs -f elastic-example-imagenet-worker-0\u001b[48D"]
[200.470343, "o", "\u001b[1C"]
[200.593949, "o", "\u001b[1C"]
[200.647547, "o", "\u001b[1C"]
[200.737522, "o", "\u001b[1C"]
[200.938547, "o", "\u001b[1C"]
[201.022688, "o", "\u001b[1C"]
[201.113337, "o", "\u001b[1C"]
[201.31895, "o", "e                                        \u001b[40D"]
[201.320195, "o", "dit pytorchjobs.kubeflow.org\u001b[28D"]
[201.516658, "o", "\u001b[1C"]
[202.315345, "o", "\u001b[27C"]
[203.011557, "o", "\u001b[?1l\u001b>"]
[203.011677, "o", "\u001b[?2004l\r\r\n"]
[203.011947, "o", "\u001bkkubectl\u001b\\"]
[203.180428, "o", "\u001b[?1049h\u001b[?1h\u001b="]
[203.180758, "o", "\u001b[1;35r\u001b[23m\u001b[24m\u001b[0m\u001b[H\u001b[J\u001b[?25l\u001b[35;1H\"/tmp/kubectl-edit-3621617296.yaml\""]
[203.18081, "o", " 62L, 2479C"]
[203.205256, "o", "\u001b[1;1H\u001b[34m# Please edit the object below. Lines beginning with a '#' will be ignored,\r\n# and an empty file will abort the edit. If an error occurs while saving this file will be\r\n# reopened with the relevant failures.\r\n#\u001b[0m\r\n\u001b[36mapiVersion\u001b[0m\u001b[35m:\u001b[0m kubeflow.org/v1\r\n\u001b[36mkind\u001b[0m\u001b[35m:\u001b[0m PyTorchJob\r\n\u001b[36mmetadata\u001b[0m\u001b[35m:\u001b[0m\r\n  \u001b[36mannotations\u001b[0m\u001b[35m:\u001b[0m\r\n    \u001b[36mkubectl.kubernetes.io/last-applied-configuration\u001b[0m\u001b[35m:\u001b[0m |\u001b[10;7H\u001b[35m{\u001b[0m\u001b[31m\"apiVersion\"\u001b[0m\u001b[35m:\u001b[0m\u001b[31m\"kubeflow.org/v1\"\u001b[0m,\u001b[31m\"kind\"\u001b[0m\u001b[35m:\u001b[0m\u001b[31m\"PyTorchJob\"\u001b[0m,\u001b[31m\"metadata\"\u001b[0m\u001b[35m:{\u001b[0m\u001b[31m\"annotations\"\u001b[0m\u001b[35m:{}\u001b[0m,\u001b[31m\"name\"\u001b[0m\u001b[35m:\u001b[0m\u001b[31m\"elastic-example-imagenet\"\u001b[0m,,\u001b[11;1H\u001b[31m\"namespace\"\u001b[0m\u001b[35m:\u001b[0m\u001b[31m\"default\"\u001b[0m\u001b[35m}\u001b[0m,\u001b[31m\"spec\"\u001b[0m\u001b[35m:{\u001b[0m\u001b[31m\"elasticPolicy\"\u001b[0m\u001b[35m:{\u001b[0m\u001b[31m\"maxReplicas\"\u001b[0m\u001b[35m:\u001b[0m2,\u001b[31m\"maxRestarts\"\u001b[0m\u001b[35m:\u001b[0m100,\u001b[31m\"minReplicas\"\u001b[0m\u001b[35m:\u001b[0m1,\u001b[31m\"rdzvBackend\"\u001b[0m\u001b[35m:\u001b[0m\u001b[31m\"c10d\"\u001b[0m\u001b[35m}\u001b[0m,\u001b[31m\"\"\u001b[12;1HpytorchReplicaSpecs\"\u001b[0m\u001b["]
[203.20539, "o", "35m:{\u001b[0m\u001b[31m\"Worker\"\u001b[0m\u001b[35m:{\u001b[0m\u001b[31m\"replicas\"\u001b[0m\u001b[35m:\u001b[0m2,\u001b[31m\"restartPolicy\"\u001b[0m\u001b[35m:\u001b[0m\u001b[31m\"OnFailure\"\u001b[0m,\u001b[31m\"template\"\u001b[0m\u001b[35m:{\u001b[0m\u001b[31m\"spec\"\u001b[0m\u001b[35m:{\u001b[0m\u001b[31m\"containers\"\u001b[0m\u001b[35m:[{\u001b[0m\u001b[31m\"command\"\u001b[0m\u001b[35m:[\u001b[0m\u001b[31m\"pp\u001b[13;1Hython\"\u001b[0m,\u001b[31m\"-m\"\u001b[0m,\u001b[31m\"torch.distributed.run\"\u001b[0m,\u001b[31m\"/workspace/examples/imagenet.py\"\u001b[0m,\u001b[31m\"--arch=resnet18\"\u001b[0m,\u001b[31m\"--epochs=20\"\u001b[0m,\u001b[31m\"--batch-size=32\"\u001b[0m,\u001b[31m\"\"\u001b[14;1H--workers=0\"\u001b[0m,\u001b[31m\"/workspace/data/tiny-imagenet-200\"\u001b[0m\u001b[35m]\u001b[0m,\u001b[31m\"env\"\u001b[0m\u001b[35m:[{\u001b[0m\u001b[31m\"name\"\u001b[0m\u001b[35m:\u001b[0m\u001b[31m\"LOGLEVEL\"\u001b[0m,\u001b[31m\"value\"\u001b[0m\u001b[35m:\u001b[0m\u001b[31m\"DEBUG\"\u001b[0m\u001b[35m}]\u001b[0m,\u001b[31m\"image\"\u001b[0m\u001b[35m:\u001b[0m\u001b[31m\"gaocegege/pytorch--\u001b[15;1Helastic-example-imagenet:1.0.0-sigterm\"\u001b[0m,\u001b[31m\"imagePullPolicy\"\u001b[0m\u001b[35m:\u001b[0m\u001b[31m\"IfNotPresent\"\u001b[0m,\u001b[31m\"name\"\u001b[0m\u001b[35m:\u001b[0m\u001b[31m\"pytorch\"\u001b[0m\u001b[35m}]}}}}}}\u001b[0m\r\n  \u001b[36mcreationTimestamp\u001b[0m\u001b[35m:\u001b[0m \u001b[31m\"2021-11-04T08:22:57Z\"\u001b[0m\r\n  \u001b[36mgeneration"]
[203.211949, "o", "\u001b[0m\u001b[35m:\u001b[0m \u001b[31m2\u001b[0m\r\n  \u001b[36mname\u001b[0m\u001b[35m:\u001b[0m elastic-example-imagenet\r\n  \u001b[36mnamespace\u001b[0m\u001b[35m:\u001b[0m default\r\n  \u001b[36mresourceVersion\u001b[0m\u001b[35m:\u001b[0m \u001b[31m\"764543\"\u001b[0m\r\n  \u001b[36muid\u001b[0m\u001b[35m:\u001b[0m 6694eace-31cf-4ec3-a86b-25d4d5c7c61c\r\n\u001b[36mspec\u001b[0m\u001b[35m:\u001b[0m\r\n  \u001b[36melasticPolicy\u001b[0m\u001b[35m:\u001b[0m\r\n    \u001b[36mmaxReplicas\u001b[0m\u001b[35m:\u001b[0m \u001b[31m2\u001b[0m\r\n    \u001b[36mmaxRestarts\u001b[0m\u001b[35m:\u001b[0m \u001b[31m100\u001b[0m\r\n    \u001b[36mminReplicas\u001b[0m\u001b[35m:\u001b[0m \u001b[31m1\u001b[0m\r\n    \u001b[36mrdzvBackend\u001b[0m\u001b[35m:\u001b[0m c10d\r\n  \u001b[36mpytorchReplicaSpecs\u001b[0m\u001b[35m:\u001b[0m\r\n    \u001b[36mWorker\u001b[0m\u001b[35m:\u001b[0m\u001b[30;7H\u001b[36mreplicas\u001b[0m\u001b[35m:\u001b[0m \u001b[31m1\u001b[0m\u001b[31;7H\u001b[36mrestartPolicy\u001b[0m\u001b[35m:\u001b[0m OnFailure\u001b[32;7H\u001b[36mtemplate\u001b[0m\u001b[35m:\u001b[0m\u001b[33;9H\u001b[36mspec\u001b[0m\u001b[35m:\u001b[0m\u001b[34;11H\u001b[36mcontainers\u001b[0m\u001b[35m:\u001b[0m\u001b[35;104H1,1\u001b[11CTop\u001b[1;1H\u001b[34h\u001b[?25h"]
[203.591258, "o", "\u001b[?25l\u001b[35;94H~@k\u001b[1;1H"]
[203.592027, "o", "\u001b[35;94H   \u001b[2;1H\u001b[35;104H2\u001b[2;1H\u001b[34h\u001b[?25h"]
[204.077805, "o", "\u001b[?25l\u001b[35;104H3\u001b[3;1H\u001b[34h\u001b[?25h\u001b[?25l\u001b[35;104H4\u001b[4;1H\u001b[34h\u001b[?25h\u001b[?25l\u001b[35;94H~@k\u001b[4;1H"]
[204.077912, "o", "\u001b[35;94H   \u001b[5;1H\u001b[35;104H5\u001b[5;1H\u001b[34h\u001b[?25h"]
[204.101954, "o", "\u001b[?25l\u001b[35;94H~@k\u001b[5;1H"]
[204.102466, "o", "\u001b[35;94H   \u001b[6;1H\u001b[35;104H6\u001b[6;1H\u001b[34h\u001b[?25h"]
[204.136471, "o", "\u001b[?25l\u001b[35;94H~@k\u001b[6;1H"]
[204.13664, "o", "\u001b[35;94H   \u001b[7;1H\u001b[35;104H7\u001b[7;1H\u001b[34h\u001b[?25h"]
[204.17767, "o", "\u001b[?25l\u001b[35;94H~@k\u001b[7;1H"]
[204.177846, "o", "\u001b[35;94H   \u001b[8;1H\u001b[35;104H8\u001b[8;1H\u001b[34h\u001b[?25h"]
[204.204515, "o", "\u001b[?25l\u001b[35;94H~@k\u001b[8;1H"]
[204.204688, "o", "\u001b[35;94H   \u001b[9;1H\u001b[35;104H9\u001b[9;1H\u001b[34h\u001b[?25h"]
[204.244205, "o", "\u001b[?25l\u001b[35;94H~@k\u001b[9;1H"]
[204.245138, "o", "\u001b[35;94H   \u001b[10;1H\u001b[35;104H10,1\u001b[10;1H\u001b[34h\u001b[?25h"]
[204.271986, "o", "\u001b[?25l\u001b[35;94H~@k\u001b[10;1H"]
[204.272862, "o", "\u001b[35;94H   \u001b[16;1H\u001b[35;105H1\u001b[16;1H\u001b[34h\u001b[?25h"]
[204.303947, "o", "\u001b[?25l\u001b[35;94H~@k\u001b[16;1H"]
[204.304475, "o", "\u001b[35;94H   \u001b[17;1H\u001b[35;105H2\u001b[17;1H\u001b[34h\u001b[?25h"]
[204.341815, "o", "\u001b[?25l\u001b[35;94H~@k\u001b[17;1H"]
[204.342047, "o", "\u001b[35;94H   \u001b[18;1H\u001b[35;105H3\u001b[18;1H\u001b[34h\u001b[?25h"]
[204.370701, "o", "\u001b[?25l\u001b[35;94H~@k\u001b[18;1H"]
[204.370845, "o", "\u001b[35;94H   \u001b[19;1H\u001b[35;105H4\u001b[19;1H\u001b[34h\u001b[?25h"]
[204.408648, "o", "\u001b[?25l\u001b[35;94H~@k\u001b[19;1H"]
[204.408814, "o", "\u001b[35;94H   \u001b[20;1H\u001b[35;105H5\u001b[20;1H\u001b[34h\u001b[?25h"]
[204.439481, "o", "\u001b[?25l\u001b[35;94H~@k\u001b[20;1H"]
[204.440353, "o", "\u001b[35;94H   \u001b[21;1H\u001b[35;105H6\u001b[21;1H\u001b[34h\u001b[?25h"]
[204.47108, "o", "\u001b[?25l\u001b[35;94H~@k\u001b[21;1H"]
[204.471296, "o", "\u001b[35;94H   \u001b[22;1H\u001b[35;105H7\u001b[22;1H\u001b[34h\u001b[?25h"]
[204.505086, "o", "\u001b[?25l\u001b[35;94H~@k\u001b[22;1H"]
[204.505583, "o", "\u001b[35;94H   \u001b[23;1H\u001b[35;105H8\u001b[23;1H\u001b[34h\u001b[?25h"]
[204.536683, "o", "\u001b[?25l\u001b[35;94H~@k\u001b[23;1H"]
[204.536847, "o", "\u001b[35;94H   \u001b[24;1H\u001b[35;105H9\u001b[24;1H\u001b[34h\u001b[?25h"]
[204.572127, "o", "\u001b[?25l\u001b[35;94H~@k\u001b[24;1H"]
[204.573411, "o", "\u001b[35;94H   \u001b[25;1H\u001b[35;104H20\u001b[25;1H\u001b[34h\u001b[?25h"]
[204.604781, "o", "\u001b[?25l\u001b[35;94H~@k\u001b[25;1H"]
[204.605444, "o", "\u001b[35;94H   \u001b[26;1H\u001b[35;105H1\u001b[26;1H\u001b[34h\u001b[?25h"]
[205.030675, "o", "\u001b[?25l\u001b[35;94H~@k\u001b[26;1H"]
[205.030828, "o", "\u001b[35;94H   \u001b[27;1H\u001b[35;105H2\u001b[27;1H\u001b[34h\u001b[?25h"]
[205.328356, "o", "\u001b[?25l\u001b[35;94H~@k\u001b[27;1H"]
[205.328517, "o", "\u001b[35;94H   \u001b[28;1H\u001b[35;105H3\u001b[28;1H\u001b[34h\u001b[?25h"]
[205.510159, "o", "\u001b[?25l\u001b[35;94H~@k\u001b[28;1H"]
[205.510321, "o", "\u001b[35;94H   \u001b[29;1H\u001b[35;105H4\u001b[29;1H\u001b[34h\u001b[?25h"]
[205.64948, "o", "\u001b[?25l\u001b[35;94H~@k\u001b[29;1H"]
[205.651739, "o", "\u001b[35;94H   \u001b[29;1H\u001b[1;34r\u001b[34;1H\r\n\u001b[1;35r\u001b[34;11H\u001b[33m- \u001b[0m\u001b[36mcommand\u001b[0m\u001b[35m:\u001b[0m\u001b[35;1H\u001b[K\u001b[35;104H25,1\u001b[11C3%\u001b[29;1H\u001b[34h\u001b[?25h"]
[205.908571, "o", "\u001b[?25l\u001b[35;94H~@k\u001b[29;1H"]
[205.908751, "o", "\u001b[35;94H   \u001b[29;2H\u001b[35;107H2\u001b[29;2H\u001b[34h\u001b[?25h"]
[206.328266, "o", "\u001b[?25l\u001b[35;94H~@k\u001b[29;2H"]
[206.32849, "o", "\u001b[35;94H   \u001b[29;3H\u001b[35;107H3\u001b[29;3H\u001b[34h\u001b[?25h"]
[206.353469, "o", "\u001b[?25l\u001b[35;94H~@k\u001b[29;3H"]
[206.353605, "o", "\u001b[35;94H   \u001b[29;4H\u001b[35;107H4\u001b[29;4H\u001b[34h\u001b[?25h"]
[206.390209, "o", "\u001b[?25l\u001b[35;94H~@k\u001b[29;4H"]
[206.39044, "o", "\u001b[35;94H   \u001b[29;5H\u001b[35;107H5\u001b[29;5H\u001b[34h\u001b[?25h"]
[206.420435, "o", "\u001b[?25l\u001b[35;94H~@k\u001b[29;5H"]
[206.420608, "o", "\u001b[35;94H   \u001b[29;6H\u001b[35;107H6\u001b[29;6H\u001b[34h\u001b[?25h"]
[206.454314, "o", "\u001b[?25l\u001b[35;94H~@k\u001b[29;6H"]
[206.454484, "o", "\u001b[35;94H   \u001b[29;7H\u001b[35;107H7\u001b[29;7H\u001b[34h\u001b[?25h"]
[206.491363, "o", "\u001b[?25l\u001b[35;94H~@k\u001b[29;7H"]
[206.491509, "o", "\u001b[35;94H   \u001b[29;8H\u001b[35;107H8\u001b[29;8H\u001b[34h\u001b[?25h"]
[206.521556, "o", "\u001b[?25l\u001b[35;94H~@k\u001b[29;8H"]
[206.522191, "o", "\u001b[35;94H   \u001b[29;9H\u001b[35;107H9\u001b[29;9H\u001b[34h\u001b[?25h"]
[206.553502, "o", "\u001b[?25l\u001b[35;94H~@k\u001b[29;9H"]
[206.553674, "o", "\u001b[35;94H   \u001b[29;10H\u001b[35;107H10\u001b[29;10H\u001b[34h\u001b[?25h"]
[206.586612, "o", "\u001b[?25l\u001b[35;94H~@k\u001b[29;10H"]
[206.586779, "o", "\u001b[35;94H   \u001b[29;11H\u001b[35;108H1\u001b[29;11H\u001b[34h\u001b[?25h"]
[206.620995, "o", "\u001b[?25l\u001b[35;94H~@k\u001b[29;11H"]
[206.621533, "o", "\u001b[35;94H   \u001b[29;12H\u001b[35;108H2\u001b[29;12H\u001b[34h\u001b[?25h"]
[206.654324, "o", "\u001b[?25l\u001b[35;94H~@k\u001b[29;12H"]
[206.654508, "o", "\u001b[35;94H   \u001b[29;13H\u001b[35;108H3\u001b[29;13H\u001b[34h\u001b[?25h"]
[206.687904, "o", "\u001b[?25l\u001b[35;94H~@k\u001b[29;13H"]
[206.688427, "o", "\u001b[35;94H   \u001b[29;14H\u001b[35;108H4\u001b[29;14H\u001b[34h\u001b[?25h"]
[206.721193, "o", "\u001b[?25l\u001b[35;94H~@k\u001b[29;14H"]
[206.721367, "o", "\u001b[35;94H   \u001b[29;15H\u001b[35;108H5\u001b[29;15H\u001b[34h\u001b[?25h"]
[206.759813, "o", "\u001b[?25l\u001b[35;94H~@k\u001b[29;15H"]
[206.760005, "o", "\u001b[35;94H   \u001b[29;16H\u001b[35;108H6\u001b[29;16H\u001b[34h\u001b[?25h"]
[206.789348, "o", "\u001b[?25l\u001b[35;94H~@k\u001b[29;16H"]
[206.789658, "o", "\u001b[35;94H   \u001b[29;17H\u001b[35;108H7\u001b[29;17H\u001b[34h\u001b[?25h"]
[206.8246, "o", "\u001b[?25l\u001b[35;94H~@k\u001b[29;17H"]
[206.825144, "o", "\u0007\u001b[35;94H   \u001b[29;17H\u001b[34h\u001b[?25h"]
[206.862398, "o", "\u001b[?25l\u001b[35;94H~@k\u001b[29;17H"]
[206.8625, "o", "\u001b[35;94H   \u001b[29;17H\u001b[34h\u001b[?25h"]
[206.892114, "o", "\u001b[?25l\u001b[35;94H~@k\u001b[29;17H"]
[206.89228, "o", "\u001b[35;94H   \u001b[29;17H\u001b[34h\u001b[?25h"]
[206.925669, "o", "\u001b[?25l\u001b[35;94H~@k\u001b[29;17H"]
[206.925914, "o", "\u001b[35;94H   \u001b[29;17H\u001b[34h\u001b[?25h"]
[206.96813, "o", "\u001b[?25l\u001b[35;94H~@k\u001b[29;17H"]
[206.969276, "o", "\u001b[35;94H   \u001b[29;17H\u001b[34h\u001b[?25h"]
[206.994181, "o", "\u001b[?25l\u001b[35;94H~@k\u001b[29;17H"]
[206.994343, "o", "\u001b[35;94H   \u001b[29;17H\u001b[34h\u001b[?25h"]
[207.029335, "o", "\u001b[?25l\u001b[35;94H~@k\u001b[29;17H"]
[207.029817, "o", "\u001b[35;94H   \u001b[29;17H\u001b[34h\u001b[?25h"]
[207.060876, "o", "\u001b[?25l\u001b[35;94H~@k\u001b[29;17H"]
[207.061037, "o", "\u001b[35;94H   \u001b[29;17H\u001b[34h\u001b[?25h"]
[207.094598, "o", "\u001b[?25l\u001b[35;94H~@k\u001b[29;17H"]
[207.094748, "o", "\u001b[35;94H   \u001b[29;17H\u001b[34h\u001b[?25h"]
[207.127794, "o", "\u001b[?25l\u001b[35;94H~@k\u001b[29;17H"]
[207.127931, "o", "\u001b[35;94H   \u001b[29;17H\u001b[34h\u001b[?25h"]
[207.167059, "o", "\u001b[?25l\u001b[35;94H~@k\u001b[29;17H"]
[207.167195, "o", "\u001b[35;94H   \u001b[29;17H\u001b[34h\u001b[?25h"]
[207.560656, "o", "\u001b[?25l\u001b[35;94Ha\u001b[29;17H"]
[207.560819, "o", "\u001b[35;94H \u001b[29;18H\u001b[35;1H\u001b[1m-- INSERT --\u001b[0m\u001b[35;104H\u001b[K\u001b[35;104H25,18\u001b[10C3%\u001b[29;18H\u001b[34h\u001b[?25h"]
[207.829275, "o", "\u001b[?25l\u001b[29;17H\u001b[K\u001b[35;108H7\u001b[29;17H\u001b[34h\u001b[?25h"]
[208.179215, "o", "\u001b[?25l\u001b[31m2\u001b[0m\u001b[35;108H8\u001b[29;18H\u001b[34h\u001b[?25h"]
[208.903405, "o", "\u001b[?25l\u001b[1;34r\u001b[34;1H\r\n\u001b[1;35r\u001b[34;13H\u001b[33m- \u001b[0mpython\u001b[35;104H\u001b[K\u001b[35;104H26,18\u001b[10C6%\u001b[29;18H\u001b[34h\u001b[?25h"]
[209.203011, "o", "\u001b[?25l\u001b[1;34r\u001b[34;1H\r\n\u001b[1;35r\u001b[34;13H\u001b[33m- \u001b[0m-m\u001b[35;104H\u001b[K\u001b[35;104H27,16\u001b[10C9%\u001b[29;16H\u001b[34h\u001b[?25h"]
[210.134196, "o", "\u001b[35;1H\u001b[K\u001b[29;15H\u001b[?25l\u001b[35;94H^[\u001b[29;15H"]
[210.234751, "o", "\u001b[35;94H  \u001b[29;16H"]
[210.23526, "o", "\u001b[35;104H27,15\u001b[10C9%\u001b[29;15H\u001b[34h\u001b[?25h"]
[211.142302, "o", "\u001b[?25l\u001b[35;94H:\u001b[29;15H\u001b[35;94H\u001b[K\u001b[35;1H:\u001b[34h\u001b[?25h"]
[211.449525, "o", "w"]
[211.756446, "o", "q"]
[212.026598, "o", "\r"]
[212.0272, "o", "\u001b[?25l\"/tmp/kubectl-edit-3621617296.yaml\""]
[212.031338, "o", " 62L, 2479C written"]
[212.032541, "o", "\r\r\r\n\u001b[?1l\u001b>\u001b[34h\u001b[?25h\u001b[?1049l"]
[212.048158, "o", "\\"]
[212.150077, "o", "pytorchjob.kubeflow.org/elastic-example-imagenet edited\r\n"]
[212.155192, "o", "\u001b[1m\u001b[3m%\u001b[23m\u001b[1m\u001b[0m                                                                                                                        \r \r"]
[212.155308, "o", "\u001bk..ning-operator\u001b\\"]
[212.199709, "o", "\r\u001b[0m\u001b[23m\u001b[24m\u001b[J(base) \u001b[01;32m➜  \u001b[36mtraining-operator\u001b[00m \u001b[01;34mgit:(\u001b[31mpytorchelastic\u001b[34m) \u001b[33m✗\u001b[00m \u001b[K"]
[212.199868, "o", "\u001b[?1h\u001b=\u001b[?2004h"]
[212.200549, "o", "\\"]
[212.988815, "o", "\b \b"]
[214.037762, "o", "k"]
[214.038792, "o", "\bkubectl edit pytorchjobs.kubeflow.org\u001b[36D"]
[214.320535, "o", "\u001b[1C"]
[214.471844, "o", "\u001b[1C"]
[214.830044, "o", "\u001b[1C"]
[215.269338, "o", "\u001b[1C"]
[215.515418, "o", "\u001b[1C"]
[215.619597, "o", "\u001b[1C"]
[215.762696, "o", "\u001b[1C"]
[216.842236, "o", "g                            \u001b[28D"]
[216.843756, "o", "et pytorchjobs -o yaml\u001b[22D"]
[216.946433, "o", "\u001b[1C"]
[217.053704, "o", "\u001b[1C"]
[217.183203, "o", "\u001b[1C"]
[218.933079, "o", "\b\b\b\b                       \u001b[23D"]
[218.934285, "o", "edit pytorchjobs.kubeflow.org\u001b[29D"]
[218.965, "o", "\u001b[8D                                     \u001b[37D"]
[219.241499, "o", "w"]
[219.242835, "o", "\bwatch kubectl get pods\u001b[21D"]
[219.244328, "o", "\bwe                    \u001b[20D"]
[219.311146, "o", "a"]
[219.680654, "o", "\b \b"]
[219.798118, "o", "\b\bw \b"]
[219.799122, "o", "\bwatch kubectl get pods\u001b[21D"]
[220.053279, "o", "\b                      \u001b[22D"]
[220.260344, "o", "w"]
[220.262364, "o", "\bwatch kubectl get pods\u001b[21D"]
[220.312554, "o", "\u001b[1C"]
[220.564243, "o", "\u001b[1C"]
[221.485518, "o", "\u001b[19C"]
[221.9003, "o", "\u001b[?1l\u001b>\u001b[?2004l\r\r\n\u001bkwatch\u001b\\"]
[222.016783, "o", "\u001b(B\u001b)0\u001b[?1049h\u001b[1;35r\u001b[m\u000f\u001b[4l\u001b[H\u001b[JEvery 2.0s: kubectl get pods\u001b[1;91Hcegao: Thu Nov  4 16:26:26 2021\u001b[3;1HNAME\u001b[3;37HREADY   STATUS    RESTARTS   AGE\r\u001b[4delastic-example-imagenet-worker-0   1/1     Running   0\u001b[4;66H3m29s\r\u001b[5delastic-example-imagenet-worker-1   1/1     Running   0\u001b[5;66H10s\u001b[35;121H"]
[224.092734, "o", "\u001b[1;116H8\u001b[4;68H31\u001b[5;67H2\u001b[35;121H"]
[226.155665, "o", "\u001b[1;115H30\u001b[4;69H3\u001b[5;67H4\u001b[35;121H"]
[226.609037, "o", "\u001b[35;1H\u001b[?1049l\r\u001b[?1l\u001b>"]
[226.609398, "o", "\u001b[1m\u001b[3m%\u001b[23m\u001b[1m\u001b[0m                                                                                                                        \r \r\u001bk..ning-operator\u001b\\"]
[226.66203, "o", "\r\u001b[0m\u001b[23m\u001b[24m\u001b[J(base) \u001b[01;32m➜  \u001b[36mtraining-operator\u001b[00m \u001b[01;34mgit:(\u001b[31mpytorchelastic\u001b[34m) \u001b[33m✗\u001b[00m \u001b[K\u001b[?1h\u001b=\u001b[?2004h"]
[226.996501, "o", "k"]
[226.997549, "o", "\bkubectl edit pytorchjobs.kubeflow.org\u001b[36D"]
[227.230598, "o", "\u001b[1C"]
[227.354808, "o", "\u001b[1C"]
[227.528037, "o", "\u001b[1C"]
[227.564201, "o", "\u001b[1C"]
[227.835368, "o", "\u001b[1C"]
[227.949403, "o", "\u001b[1C"]
[228.02685, "o", "\u001b[1C"]
[228.244899, "o", "l                            \u001b[28D"]
[228.245769, "o", "ogs -f elastic-example-imagenet-worker-0\u001b[40D"]
[228.365589, "o", "\u001b[1C"]
[228.652838, "o", "\u001b[1C"]
[228.688823, "o", "\u001b[1C"]
[228.740736, "o", "\u001b[1C"]
[229.6848, "o", "\u001b[36C"]
[229.962192, "o", "\b \b"]
[229.964545, "o", "0\b"]
[230.12102, "o", "1"]
[230.617885, "o", "\u001b[?1l\u001b>\u001b[?2004l\r\r\n\u001bkkubectl\u001b\\"]
[230.716297, "o", "INFO:torch.distributed.launcher.api:Starting elastic_operator with launch configs:\r\n"]
[230.716608, "o", "  entrypoint       : /workspace/examples/imagenet.py\r\n  min_nodes        : 1\r\n  max_nodes        : 2\r\n  nproc_per_node   : 1\r\n  run_id           : none\r\n"]
[230.717476, "o", "  rdzv_backend     : c10d\r\n  rdzv_endpoint    : elastic-example-imagenet-worker-0:23456\r\n  rdzv_configs     : {'timeout': 900}\r\n  max_restarts     : 100\r\n  monitor_interval : 5\r\n  log_dir          : None\r\n"]
[230.717499, "o", "  metrics_cfg      : {}\r\n\r\nINFO:torch.distributed.elastic.agent.server.local_elastic_agent:log directory set to: /tmp/torchelastic_msgofv1s/none_zeo41h08\r\nINFO:torch.distributed.elastic.agent.server.api:[default] starting workers for entrypoint: python\r\nINFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous'ing worker group\r\nINFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous complete for workers. Result:\r\n  restart_count=0\r\n  master_addr=elastic-example-imagenet-worker-0\r\n  master_port=59519\r\n  group_rank=1\r\n  group_world_size=2\r\n  local_ranks=[0]\r\n  role_ranks=[1]\r\n  global_ranks=[1]\r\n  role_world_sizes=[2]\r\n  global_world_sizes=[2]\r\n\r\nINFO:torch.distributed.elastic.agent.server.api:[default] Starting worker group"]
[230.717546, "o", "\r\nINFO:torch.distributed.elastic.multiprocessing:Setting worker0 reply file to: /tmp/torchelastic_msgofv1s/none_zeo41h08/attempt_0/0/error.json\r\n=> creating model: resnet18\r\n=> no workers have checkpoints, starting from epoch 0\r\n=> start_epoch: 0, best_acc1: 0\r\nEpoch: [0][   0/1563]\tTime  1.978 ( 1.978)\tData  0.051 ( 0.051)\tLoss 7.0423e+00 (7.0423e+00)\tAcc@1   0.00 (  0.00)\tAcc@5   0.00 (  0.00)\r\n"]
[240.021788, "o", "Epoch: [0][  10/1563]\tTime  1.927 ( 1.980)\tData  0.037 ( 0.051)\tLoss 7.2238e+00 (6.7294e+00)\tAcc@1   0.00 (  0.57)\tAcc@5   6.25 (  2.84)\r\n"]
[241.317136, "o", "^C"]
[241.319542, "o", "\r\n\u001b[1m\u001b[3m%\u001b[23m\u001b[1m\u001b[0m                                                                                                                        \r \r\u001bk..ning-operator\u001b\\"]
[241.371852, "o", "\r\u001b[0m\u001b[23m\u001b[24m\u001b[J(base) \u001b[01;31m➜  \u001b[36mtraining-operator\u001b[00m \u001b[01;34mgit:(\u001b[31mpytorchelastic\u001b[34m) \u001b[33m✗\u001b[00m \u001b[K\u001b[?1h\u001b=\u001b[?2004h"]
[242.379997, "o", "k"]
[242.383416, "o", "\bkubectl logs -f elastic-example-imagenet-worker-1\u001b[48D"]
[242.581238, "o", "\u001b[1C"]
[242.683565, "o", "\u001b[1C"]
[242.888577, "o", "\u001b[1Cectl logs -f elastic-example-imagenet-worker-1\u001b[46Dc                                             \u001b[45D"]
[242.889537, "o", "tl logs -f elastic-example-imagenet-worker-1\u001b[44D"]
[243.194741, "o", "\u001b[1C"]
[243.403005, "o", "\u001b[1C"]
[243.710079, "o", "\u001b[1C"]
[244.326088, "o", "g                                        \u001b[40D"]
[244.329317, "o", "et pytorchjobs -o yaml\u001b[22D"]
[244.34755, "o", "\u001b[1C"]
[244.732245, "o", "\b                      \u001b[22D"]
[244.733604, "o", "et pytorchjobs -o yaml\u001b[22D"]
[244.821347, "o", "\b                       \u001b[23D"]
[244.82274, "o", "logs -f elastic-example-imagenet-worker-1\u001b[41D"]
[245.34332, "o", "\u001b[41C"]
[245.635895, "o", "\b \b"]
[245.636194, "o", "1\b"]
[246.045782, "o", "0"]
[246.325658, "o", "\u001b[?1l\u001b>"]
[246.325843, "o", "\u001b[?2004l\r\r\n"]
[246.326293, "o", "\u001bkkubectl\u001b\\"]
[246.425791, "o", "INFO:torch.distributed.launcher.api:Starting elastic_operator with launch configs:\r\n  entrypoint       : /workspace/examples/imagenet.py\r\n  min_nodes        : 1\r\n  max_nodes        : 2\r\n  nproc_per_node   : 1\r\n  run_id           : none\r\n  rdzv_backend     : c10d\r\n  rdzv_endpoint    : elastic-example-imagenet-worker-0:23456\r\n  rdzv_configs     : {'timeout': 900}\r\n  max_restarts     : 100\r\n  monitor_interval : 5\r\n  log_dir          : None\r\n  metrics_cfg      : {}\r\n\r\nINFO:torch.distributed.elastic.agent.server.local_elastic_agent:log directory set to: /tmp/torchelastic_m8me90uk/none_qs2bblei\r\nINFO:torch.distributed.elastic.agent.server.api:[default] starting workers for entrypoint: python\r\nINFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous'ing worker group\r\nINFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous complete for workers. Result:\r\n  restart_count=0\r\n  master_addr=elastic-example-imagenet-worker-0\r\n  master_port=49951\r\n  group_rank=0\r\n"]
[246.426588, "o", "  group_world_size=2\r\n  local_ranks=[0]\r\n  role_ranks=[0]\r\n  global_ranks=[0]\r\n  role_world_sizes=[2]\r\n  global_world_sizes=[2]\r\n\r\nINFO:torch.distributed.elastic.agent.server.api:[default] Starting worker group\r\nINFO:torch.distributed.elastic.multiprocessing:Setting worker0 reply file to: /tmp/torchelastic_m8me90uk/none_qs2bblei/attempt_0/0/error.json\r\nINFO:torch.distributed.elastic.agent.server.api:[default] Detected 1 new nodes from group_rank=0; will restart worker group\r\nINFO:torch.distributed.elastic.agent.server.api:[default] Stopping worker group\r\nWARNING:torch.distributed.elastic.multiprocessing.api:Sending process 12 closing signal SIGTERM\r\nINFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous'ing worker group\r\nINFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous complete for workers. Result:\r\n  restart_count=0\r\n  master_addr=elastic-example-imagenet-worker-0\r\n  master_port=35023\r\n  group_rank=0\r\n  group_world_size=2\r\n  local_ranks=[0]\r\n"]
[246.426704, "o", "  role_ranks=[0]\r\n  global_ranks=[0]\r\n  role_world_sizes=[2]\r\n  global_world_sizes=[2]\r\n\r\nINFO:torch.distributed.elastic.agent.server.api:[default] Starting worker group\r\nINFO:torch.distributed.elastic.multiprocessing:Setting worker0 reply file to: /tmp/torchelastic_m8me90uk/none_qs2bblei/attempt_0/0/error.json\r\nINFO:torch.distributed.elastic.agent.server.api:[default] Detected 1 new nodes from group_rank=0; will restart worker group\r\nINFO:torch.distributed.elastic.agent.server.api:[default] Stopping worker group\r\nWARNING:torch.distributed.elastic.multiprocessing.api:Sending process 17 closing signal SIGTERM\r\nINFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous'ing worker group\r\nINFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous complete for workers. Result:\r\n  restart_count=0\r\n  master_addr=elastic-example-imagenet-worker-0\r\n"]
[246.428491, "o", "  master_port=51241\r\n  group_rank=0\r\n  group_world_size=2\r\n  local_ranks=[0]\r\n  role_ranks=[0]\r\n"]
[246.42854, "o", "  global_ranks=[0]\r\n  role_world_sizes=[2]\r\n  global_world_sizes=[2]\r\n\r\nINFO:torch.distributed.elastic.agent.server.api:[default] Starting worker group\r\nINFO:torch.distributed.elastic.multiprocessing:Setting worker0 reply file to: /tmp/torchelastic_m8me90uk/none_qs2bblei/attempt_0/0/error.json\r\nINFO:torch.distributed.elastic.agent.server.api:[default] Detected 1 new nodes from group_rank=0; will restart worker group\r\nINFO:torch.distributed.elastic.agent.server.api:[default] Stopping worker group\r\nWARNING:torch.distributed.elastic.multiprocessing.api:Sending process 22 closing signal SIGTERM\r\nINFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous'ing worker group\r\nINFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous complete for workers. Result:\r\n  restart_count=0\r\n  master_addr=elastic-example-imagenet-worker-0\r\n  master_port=52111\r\n  group_rank=0\r\n  group_world_size=2\r\n"]
[246.430167, "o", "  local_ranks=[0]\r\n  role_ranks=[0]\r\n  global_ranks=[0]\r\n  role_world_sizes=[2]\r\n  global_world_sizes=[2]\r\n\r\n"]
[246.43027, "o", "INFO:torch.distributed.elastic.agent.server.api:[default] Starting worker group\r\nINFO:torch.distributed.elastic.multiprocessing:Setting worker0 reply file to: /tmp/torchelastic_m8me90uk/none_qs2bblei/attempt_0/0/error.json\r\n=> creating model: resnet18\r\n=> no workers have checkpoints, starting from epoch 0\r\n=> start_epoch: 0, best_acc1: 0\r\nEpoch: [0][   0/1563]\tTime  2.085 ( 2.085)\tData  0.097 ( 0.097)\tLoss 7.0790e+00 (7.0790e+00)\tAcc@1   0.00 (  0.00)\tAcc@5   0.00 (  0.00)\r\nEpoch: [0][  10/1563]\tTime  1.968 ( 1.946)\tData  0.064 ( 0.051)\tLoss 6.6233e+00 (6.7385e+00)\tAcc@1   0.00 (  0.28)\tAcc@5   0.00 (  1.99)\r\nEpoch: [0][  20/1563]\tTime  1.877 ( 1.924)\tData  0.044 ( 0.047)\tLoss 5.8902e+00 (6.5127e+00)\tAcc@1   3.12 (  0.30)\tAcc@5   6.25 (  2.98)\r\nEpoch: [0][  30/1563]\tTime  1.936 ( 1.925)\tData  0.050 ( 0.045)\tLoss 5.8298e+00 (6.3235e+00)\tAcc@1   0.00 (  0.60)\tAcc@5   0.00 (  4.03)\r\nEpoch: [0][  40/1563]\tTime  1.911 ( 1.925)\tData  0.037 ( 0.046)\tLoss 5.4624e+00 (6.1743e+00)\tAcc@1   6.25 (  0.76)\tAcc@5   6.25 (  "]
[246.430333, "o", "4.57)\r\nTraceback (most recent call last):\r\n  File \"/workspace/examples/imagenet.py\", line 580, in <module>\r\n    main()\r\n  File \"/opt/conda/lib/python3.7/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 345, in wrapper\r\n    return f(*args, **kwargs)\r\n  File \"/workspace/examples/imagenet.py\", line 181, in main\r\n    train(train_loader, model, criterion, optimizer, epoch, print_freq)\r\n  File \"/workspace/examples/imagenet.py\", line 442, in train\r\n    output = model(images)\r\n  File \"/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\r\n    return forward_call(*input, **kwargs)\r\n  File \"/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/distributed.py\", line 878, in forward\r\n    self._sync_params()\r\n  File \"/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/distributed.py\", line 1382, in _sync_params\r\n    authoritative_rank,\r\n  File \"/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/distributed.py\", line 1335, in _distributed"]
[246.430346, "o", "_broadcast_coalesced\r\n    self.process_group, tensors, buffer_size, authoritative_rank\r\nRuntimeError: [/opt/conda/conda-bld/pytorch_1634272168290/work/third_party/gloo/gloo/transport/tcp/pair.cc:589] Read error [10.244.0.47]:57480: Connection reset by peer\r\nDEBUG:torch.distributed.elastic.multiprocessing.errors:User process failed with error data: {\r\n  \"message\": {\r\n    \"message\": \"RuntimeError: [/opt/conda/conda-bld/pytorch_1634272168290/work/third_party/gloo/gloo/transport/tcp/pair.cc:589] Read error [10.244.0.47]:57480: Connection reset by peer\",\r\n    \"extraInfo\": {\r\n      \"py_callstack\": \"Traceback (most recent call last):\\n  File \\\"/opt/conda/lib/python3.7/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\\\", line 345, in wrapper\\n    return f(*args, **kwargs)\\n  File \\\"/workspace/examples/imagenet.py\\\", line 181, in main\\n    train(train_loader, model, criterion, optimizer, epoch, print_freq)\\n  File \\\"/workspace/examples/imagenet.py\\\", line 442, in train\\n    output = model(imag"]
[246.430358, "o", "es)\\n  File \\\"/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\\\", line 1102, in _call_impl\\n    return forward_call(*input, **kwargs)\\n  File \\\"/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/distributed.py\\\", line 878, in forward\\n    self._sync_params()\\n  File \\\"/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/distributed.py\\\", line 1382, in _sync_params\\n    authoritative_rank,\\n  File \\\"/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/distributed.py\\\", line 1335, in _distributed_broadcast_coalesced\\n    self.process_group, tensors, buffer_size, authoritative_rank\\nRuntimeError: [/opt/conda/conda-bld/pytorch_1634272168290/work/third_party/gloo/gloo/transport/tcp/pair.cc:589] Read error [10.244.0.47]:57480: Connection reset by peer\\n\",\r\n      \"timestamp\": \"1636014295\"\r\n    }\r\n  }\r\n}\r\nERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 27) of binary: /opt/conda/bin/python\r\nINFO:torch.distributed.elastic.agent.server.api:[def"]
[246.430731, "o", "ault] Worker group FAILED. 100/100 attempts left; will restart worker group\r\nINFO:torch.distributed.elastic.agent.server.api:[default] Stopping worker group\r\nINFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous'ing worker group\r\nINFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous complete for workers. Result:\r\n  restart_count=1\r\n  master_addr=elastic-example-imagenet-worker-0\r\n  master_port=48739\r\n  group_rank=0\r\n  group_world_size=1\r\n  local_ranks=[0]\r\n  role_ranks=[0]\r\n  global_ranks=[0]\r\n  role_world_sizes=[1]\r\n  global_world_sizes=[1]\r\n\r\nINFO:torch.distributed.elastic.agent.server.api:[default] Starting worker group\r\n"]
[246.43267, "o", "INFO:torch.distributed.elastic.multiprocessing:Setting worker0 reply file to: /tmp/torchelastic_m8me90uk/none_qs2bblei/attempt_1/0/error.json\r\n=> creating model: resnet18\r\n=> no workers have checkpoints, starting from epoch 0\r\n=> start_epoch: 0, best_acc1: 0\r\nEpoch: [0][   0/3125]\tTime  1.112 ( 1.112)\tData  0.041 ( 0.041)\tLoss 7.1575e+00 (7.1575e+00)\tAcc@1   0.00 (  0.00)\tAcc@5   0.00 (  0.00)\r\nEpoch: [0][  10/3125]\tTime  1.030 ( 1.092)\tData  0.030 ( 0.032)\tLoss 7.0225e+00 (7.1951e+00)\tAcc@1   3.12 (  0.85)\tAcc@5   3.12 (  2.84)\r\nEpoch: [0][  20/3125]\tTime  1.186 ( 1.067)\tData  0.033 ( 0.031)\tLoss 6.7113e+00 (7.0286e+00)\tAcc@1   0.00 (  0.89)\tAcc@5   0.00 (  2.53)\r\nEpoch: [0][  30/3125]\tTime  1.052 ( 1.070)\tData  0.029 ( 0.030)\tLoss 5.9791e+00 (6.8319e+00)\tAcc@1   3.12 (  0.71)\tAcc@5   3.12 (  2.22)\r\nINFO:torch.distributed.elastic.agent.server.api:[default] Detected 1 new nodes from group_rank=0; will restart worker group\r\nINFO:torch.distributed.elastic.agent.server.api:[default] Stopping worker group\r\nWARNIN"]
[246.432777, "o", "G:torch.distributed.elastic.multiprocessing.api:Sending process 47 closing signal SIGTERM\r\nINFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous'ing worker group\r\nINFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous complete for workers. Result:\r\n  restart_count=1\r\n  master_addr=elastic-example-imagenet-worker-0\r\n  master_port=59519\r\n  group_rank=0\r\n  group_world_size=2\r\n  local_ranks=[0]\r\n  role_ranks=[0]\r\n  global_ranks=[0]\r\n  role_world_sizes=[2]\r\n  global_world_sizes=[2]\r\n\r\nINFO:torch.distributed.elastic.agent.server.api:[default] Starting worker group\r\nINFO:torch.distributed.elastic.multiprocessing:Setting worker0 reply file to: /tmp/torchelastic_m8me90uk/none_qs2bblei/attempt_1/0/error.json\r\n=> creating model: resnet18\r\n=> no workers have checkpoints, starting from epoch 0\r\n=> start_epoch: 0, best_acc1: 0\r\nEpoch: [0][   0/1563]\tTime  1.980 ( 1.980)"]
[246.433256, "o", "\tData  0.068 ( 0.068)\tLoss 7.0283e+00 (7.0283e+00)\tAcc@1   0.00 (  0.00)\tAcc@5   0.00 (  0.00)\r\nEpoch: [0][  10/1563]\tTime  1.912 ( 1.978)\tData  0.038 ( 0.047)\tLoss 6.6860e+00 (6.7348e+00)\tAcc@1   0.00 (  0.85)\tAcc@5   0.00 (  2.27)\r\n"]
[259.421775, "o", "Epoch: [0][  20/1563]\tTime  1.940 ( 1.960)\tData  0.039 ( 0.047)\tLoss 5.7531e+00 (6.5134e+00)\tAcc@1   3.12 (  1.19)\tAcc@5   3.12 (  2.68)\r\n"]
[268.491473, "o", "^C"]
[268.494242, "o", "\r\n"]
[268.494282, "o", "\u001b[1m\u001b[3m%\u001b[23m\u001b[1m\u001b[0m                                                                                                                        \r \r"]
[268.494884, "o", "\u001bk..ning-operator\u001b\\"]
[268.545733, "o", "\r\u001b[0m\u001b[23m\u001b[24m\u001b[J(base) \u001b[01;31m➜  \u001b[36mtraining-operator\u001b[00m \u001b[01;34mgit:(\u001b[31mpytorchelastic\u001b[34m) \u001b[33m✗\u001b[00m \u001b[K"]
[268.546024, "o", "\u001b[?1h\u001b=\u001b[?2004h"]
[269.571806, "o", "e"]
[269.573061, "o", "\bexit\b\b\b"]
[269.839938, "o", "\u001b[1Cxit\b\b\bi  \b\b"]
[269.841066, "o", "t\b"]
[270.022203, "o", "\u001b[1C"]
[270.476289, "o", "\u001b[?1l\u001b>\u001b[?2004l\r\r\n\u001bkexit\u001b\\"]
